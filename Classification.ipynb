{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Supervised Learning - Text Classification</center>\n",
    "References:\n",
    "* http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Finally, we come to machine learning ...\n",
    "<img src='https://res.cloudinary.com/practicaldev/image/fetch/s--_jRAhLTB--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://thepracticaldev.s3.amazonaws.com/i/7brl707yigrhno91vc6d.jpg' width = \"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Review basic concepts of machine learning\n",
    "  * Cross validation\n",
    "  * Performance metrics: recall and precision, AUC, PRC\n",
    "* Text Classification  \n",
    "  * Assign a document into one  or more pre-defined categories (or labels)\n",
    "  * **Single-label**: e.g. spam dection, sentiment detection\n",
    "  * **Multi-label**: e.g. news categorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Review basic concepts of machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Model assessment and selection - How valid is a model? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Generalization: the prediction capability of a model ($f$) on independent test data, \n",
    "    - Given testing samples ($X, Y$), and prediction $\\hat{Y} = f(X, \\theta)$), where $\\theta$ is the model (hyper) parameters, e.g. $K$ in KNN \n",
    "    - Prediction error (or `loss function`): $L(Y, \\hat{Y})$, e.g.\n",
    "        - Squared error: $L(Y, \\hat{Y}) = (Y-\\hat{Y})^2$\n",
    "        - Absolute error: $L(Y, \\hat{Y}) = |Y-\\hat{Y}|$\n",
    "  \n",
    "- Data-rich situation: split data into training, validation, and test sets (e.g. 50%, 25%, 25%)\n",
    "  - **Training** set: fit the model\n",
    "  - **Validation** set (sometimes called evaluation set): \n",
    "     - tune model parameters\n",
    "     - estimate prediction error for model selection\n",
    "  - **Test** set: assess the prediction erorr of the final chosen model\n",
    "     - in machine learning competition, usually a test set is given without truth labels\n",
    "     \n",
    "  <img src=\"train_validation_test.png\" width=\"40%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Cross Validation\n",
    "- However, labeled data is always scarce. We cannot afford to set aside a validation set\n",
    "- `K-fold cross validation`: \n",
    "    1. Data is separated into k subsets. Each time, one of the subsets is held as the validation set (a.k.a holdout, test set) and the rest of them is used as the training set. \n",
    "    <img src=\"cross_validation.png\" width=\"40%\"> [source] (http://spark-public.s3.amazonaws.com/nlp/slides/sentiment.pptx)\n",
    "    2. This method repeats *k* times and each time with a different subset as the test set. \n",
    "    3. Calculate average prediction error `(CV)` on `K` validation sets $$ CV(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N}{L~(~y_i,~f^{k(i)}(x_i, \\theta)~)}$$ where:\n",
    "       - $\\theta$: the model parameters (e.g. the number of neighbours in $k$-NN), \n",
    "       - $f^{k(i)}$: model fitted on the $k$th iteration, \n",
    "       - $N$: number of samples\n",
    "    4. Tune model parameters $(~\\theta~)$ to minize the average prediction error\n",
    "       - How? e.g. How to determine $K$ in KNN?\n",
    "    5. Select the model with the minimal prediction error (along with $\\theta$ determined)\n",
    "    6. Fit the selected model to all the data\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Performance metrics and model evaluation\n",
    "- **Accuracy** is often used to measure model performance\n",
    "- $ Accurarcy = \\frac{\\#~correct~predictions}{\\#~of~total~samples}$\n",
    "- However, for an imbalanced classification problem where `one category represents the overwhelming majority of the data points`, accuracy can be a problematic metric\n",
    "  - e.g. prediction of rare diseases: Assume only 1 patient in every 1M population. What is the accuracry if you predict everyone is healthy?\n",
    "  - e.g. prediction of earthquakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### 2.3.1 Precision, Recall, and F-score\n",
    "  * Precision: precentage of true cases among the predicated true cases\n",
    "  * Recall:  precentage of true cases that have been retrieved over the total number of true cases\n",
    "  * F-score: $$\\frac{2*precision*recall}{precision+recall}$$\n",
    "  * Example: \n",
    "  \n",
    "     Confusion Matrix: <img src=\"confusion_matrix.png\" width=\"50%\">\n",
    "    * For \"YES\" group: \n",
    "      - precision=?, \n",
    "      - recall=?, \n",
    "      - f-score=?\n",
    "    * For \"NO\" group:\n",
    "      - precision=?, \n",
    "      - recall=?, \n",
    "      - f-score=?\n",
    "  * Overall model performance\n",
    "    * precision_macro (or recall_macro or f1_macro) is calculated as:\n",
    "      1. calculate precision for each label\n",
    "      2. average over labels \n",
    "    * precision_micro (or recall_micro or f1_micro): calculates metrics globally regardless of labels. \n",
    "      - precision_micro = recall_micro = accuracy\n",
    "      - Note precision_micro and recall_micro are the same for single label classification\n",
    "    * With inbalanced classes, the difference between these two metrics may be significant, e.g.\n",
    "    - class 1: 950 samples, precision 90%\n",
    "    - class 2: 50 samples, precision 70%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2. True Positive `(TP)` Rate and False Positive `(FP)` Rate\n",
    "- Two types of prediction errors:\n",
    "  - False Positive: Predict a sample is positive when it is in fact negative.\n",
    "  - False Negative: Predict a sample is negative when in fact it is positive.\n",
    "\n",
    "<img src=\"classification_error.png\" width=\"60%\">\n",
    "\n",
    "- Metrics: <br>\n",
    "   $ \n",
    "    \\begin{align}\n",
    "     precision~of~positive~class &= \\frac{true~positives}{ true~positives + false~positives} \\\\\n",
    "     recall~of~positive~class &= \\frac{true~positives}{ true~positives + false~negatives} \\\\\n",
    "     true~positive~rate~(TPR) &= recall~of~positive~class  &= sensitivity\\\\\n",
    "     false~positive~rate~(FPR) &= \\frac{false~positives}{ false~positives + true~negatives} &= 1-Specificity\\\\\n",
    "     & = 1- recall~of~negative~class\n",
    "    \\end{align}\n",
    "    $\n",
    "    - Note, $false~positive~rate \\neq 1-true~positive~rate$\n",
    "- Now calculate TPR and FPR of the the image below:\n",
    "<img src=\"confusion_matrix.png\" width=\"50%\">\n",
    "- For more details, read https://towardsdatascience.com/beyond-accuracy-precision-and-recall-3da06bea9f6c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Two types of predictions in classification problems:\n",
    "   - Predict class labels directly, e.g. report the class of the majority of K nearest neighbors in KNN\n",
    "   - Predict a **probability** (i.e. confidence) for each class, e.g. the percentage of neighbors of a class \n",
    "      - Calibrate a probability threshold to determine class labels for desired performance levels\n",
    "      - How the threshold can affect precision, recall, true positive rate, and false positive rate? e.g. threshold 0.90 vs. 0.10\n",
    "      \n",
    "<img src = \"threshold_table.png\" width=\"60%\">\n",
    "\n",
    "- Example:\n",
    "  - $threshold = 0.5$: $ precision = \\frac{42}{42+16} =0.724$, $ recall = tpr = \\frac{42}{42+13} = 0.76$, $ fpr = \\frac{16}{29+16} = 0.36$\n",
    "  - $threshold = 0.9 $: $ precision = \\frac{12}{12+3} =0.8$, $ recall = tpr = \\frac{12}{12+40} = 0.23$, $ fpr = \\frac{3}{3+45} = 0.0625$\n",
    "  - As the threshold increases, for the positive class, how does each of the metric change?\n",
    "    - precision?\n",
    "    - recall or true positive rate (tpr)?\n",
    "    - false positive rate (fpr)?\n",
    "\n",
    "<img src=\"all_metrics.png\" width = \"50%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Trade-offs:\n",
    "  - Precision vs. recall (high prcision but low recall, and vice versa)\n",
    "    -  As the threshold increases, precision increases but recall decreases\n",
    "  - True positive rate vs. false positive rate \n",
    "    -  As the threshold increases, both tpr and fpr decrease. Ideally, we like to have high tpr but low fpr\n",
    "  - In real classification problems, you may prefer one metric to the other\n",
    "    - e.g. earthquake prediction\n",
    "    - e.g. desease prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 AUC and Precision-Recall Curve\n",
    "- **Receiver operating characteristic (ROC) curve**: plots the true positive rate (TPR) vs the false positive rate (FPR) as a function of the model’s threshold for classifying positives\n",
    "<img src=\"roc.png\" width=\"40%\">\n",
    "- **Area under the curve (AUC)**: metric to calculate the overall performance of a classification model based on area under the ROC curve \n",
    "   - Which model is better?\n",
    "   - How to determine best threshold?\n",
    "  <img src=\"auc_curve.png\" with=\"50%\">\n",
    "- **Precision-Recall Curve** : plot the precision (y-axis) and the recall (x-axis) for different thresholds, much like the ROC curve.\n",
    "<img src=\"precision_recall_curve.png\" width=\"50%\">\n",
    "  - Which algorithm is better?\n",
    "<img src=\"precision_recall.png\" width=\"50%\">\n",
    " source: https://i.stack.imgur.com/T0kQr.png\n",
    " \n",
    "- More about ROC and PRC: \n",
    "  - When the dataset is extremely imbalanced, i.e. the majority is negative samples, a model with high AUC is not necessarily good. For experiment, check https://www.kaggle.com/lct14558/imbalanced-data-why-you-should-not-use-roc-curve\n",
    "  - For in-depth discussion between ROC and PRC, read Davis, J. and Goadrich, M., The relationship between Precision-Recall and ROC curves. http://pages.cs.wisc.edu/~jdavis/davisgoadrichcamera2.pdf\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Problem formulation:\n",
    "    * Input: \n",
    "      - A document $d$ \n",
    "      - A fixed set of classes C = {$c_1$, $c_2$,..., $c_J$}\n",
    "      - A training set of $m$ hand-labeled documents ($d_1,c_1$),....,($d_m,c_m$)\n",
    "    * Output: a classifier that predicts $d$ to some classes $c$ $\\subset$ C\n",
    "* Basic process\n",
    "  1. Load and preprocess sample data\n",
    "  2. Extract features: e.g. bag of words with TF-IDF weights\n",
    "  3. Split feature space into trainning and test sets following cross validation method\n",
    "  4. Train a classifier/model with the training dataset using selected classification algorithm for each fold\n",
    "  5. Calculate performance\n",
    " \n",
    "* Considerations for deciding text classification algorithms\n",
    "  - should be effective in high dimensional spaces (`curse of dimensionality`)\n",
    "  - should be effective even if `the number of features is greater than the number of samples`\n",
    "  - some good algorithms to start with:\n",
    "      - Naive Bayes (https://web.stanford.edu/~jurafsky/slp3/slides/7_NB.pdf): baseline for performance benchmarking of text classification algorithms. **Read Pages 13-35 for a good understanding of naive Bayes algorithm**\n",
    "      - Support Vector Machine (SVM). References:\n",
    "        - http://www.robots.ox.ac.uk/~az/lectures/ml/lect2.pdf. **Read pages 2-4, 9, 12-19, and 29-38 for a good understanding of SVM**.\n",
    "        - https://www.svm-tutorial.com/2014/11/svm-understanding-math-part-1/\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "                    \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.1.: Load data \n",
    "# Load datasets (http://qwone.com/~jason/20Newsgroups/)\n",
    "# For convenience, a subset of the data has been saved into \"twenty_news_data.csv\"\n",
    "\n",
    "import pandas as pd\n",
    "data=pd.read_csv(\"../../dataset/twenty_news_data.csv\",header=0)\n",
    "data.head()\n",
    "\n",
    "# print out the full text of the first sample\n",
    "print(data[\"text\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. TF-IDF matrix generation\n",
    "- Function: **sklearn.feature_extraction.text.TfidfVectorizer**(input='content',encoding='utf-8', decode_error='strict', token_pattern='(?u)\\b\\w\\w+\\b', lowercase=True, stop_words=None, ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None, norm='l2', use_idf=True, smooth_idf=True, ...)\n",
    "- Some useful parameters:\n",
    "    * **input** : string {'filename', 'file', 'content').\n",
    "    * **token_pattern** : Regular expression denoting what constitutes a “token”. The default is '(?u)\\b\\w\\w+\\b', i.e. a token contains at least two word characters in unicode (note: ?u: unicode, \\b: space or non-word character, i.e. boundary, \\w: word character). \n",
    "    * **ngram_range** : tuple (min_n, max_n): The lower and upper boundary of the range of n-values for different n-grams to be extracted. \n",
    "    * **stop_words** : string {‘english’}, list, or None (default)\n",
    "    * **lowercase** : boolean, default True: Convert all characters to lowercase before tokenizing.\n",
    "    * **max_df/min_df** : float in range [0.0, 1.0] or int, default=1.0: When building the vocabulary ignore terms that have a document frequency strictly higher (lower) than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts. \n",
    "    * **max_features** : int or None, default=None. If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n",
    "    * **norm** : 'l1', 'l2' or None, optional. Norm used to normalize term vectors. None for no normalization.\n",
    "    * **use_idf** : boolean, default=True. Enable inverse-document-frequency reweighting.\n",
    "    * **smooth_idf** : boolean, default=True. Smooth idf weights by adding one to document frequencies, as if an extra document was seen containing every term in the collection exactly once. Prevents zero divisions.\n",
    "    * **binary** : If True, all non-zero term counts are set to 1. This does not mean outputs will have only 0/1 values, only that the tf term in tf-idf is binary. (Set idf and normalization to False to get 0/1 outputs).\n",
    "- For all the parameters, see http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.2 Create TF-IDF Matrix\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# initialize the TfidfVectorizer \n",
    "\n",
    "tfidf_vect = TfidfVectorizer() \n",
    "\n",
    "# with stop words removed\n",
    "#tfidf_vect = TfidfVectorizer(stop_words=\"english\") \n",
    "\n",
    "# generate tfidf matrix\n",
    "dtm= tfidf_vect.fit_transform(data[\"text\"])\n",
    "\n",
    "print(\"type of dtm:\", type(dtm))\n",
    "print(\"size of tfidf matrix:\", dtm.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.3. Examine TF-IDF\n",
    "\n",
    "# 1. Check vocabulary\n",
    "\n",
    "# Vocabulary is a dictionary mapping a word to an index\n",
    "\n",
    "# the number of words in the vocabulary\n",
    "print(\"total number of words:\", len(tfidf_vect.vocabulary_))\n",
    "\n",
    "print(\"type of vocabulary:\", \\\n",
    "      type(tfidf_vect.vocabulary_))\n",
    "print(\"index of word 'city' in vocabulary:\", \\\n",
    "      tfidf_vect.vocabulary_['city'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4 check words with top tf-idf wights in a document, \n",
    "# e.g. 1st document\n",
    "\n",
    "# get mapping from word index to word\n",
    "# i.e. reversal mapping of tfidf_vect.vocabulary_\n",
    "voc_lookup={tfidf_vect.vocabulary_[word]:word \\\n",
    "            for word in tfidf_vect.vocabulary_}\n",
    "\n",
    "print(\"\\nOriginal text: \\n\"+data[\"text\"][0])\n",
    "\n",
    "print(\"\\ntfidf weights: \\n\")\n",
    "\n",
    "# first, covert the sparse matrix row to a dense array\n",
    "doc0=dtm[0].toarray()[0]\n",
    "print(\"Vectorized document shape: \", doc0.shape, \"\\n\")\n",
    "\n",
    "# get index of top 20 words\n",
    "print(\"top words:\")\n",
    "top_words=(doc0.argsort())[::-1][0:20]\n",
    "for i in top_words:\n",
    "    print(\"{0}:\\t{1:.3f}\".format(voc_lookup[i], doc0[i]))\n",
    "#[(voc_lookup[i], '%.3f'%doc0[i]) for i in top_words]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.5. classification using a single fold\n",
    "\n",
    "# use MultinomialNB algorithm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# import method for split train/test data set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import method to calculate metrics\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# split dataset into train (70%) and test sets (30%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\\\n",
    "                dtm, data[\"label\"], test_size=0.3,\\\n",
    "                    random_state=0)\n",
    "\n",
    "# train a multinomial naive Bayes model using the testing data\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "# predict the news group for the test dataset\n",
    "predicted=clf.predict(X_test)\n",
    "\n",
    "# check a few samples\n",
    "predicted[0:3]\n",
    "y_test[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.6. Performance evaluation: \n",
    "# precision, recall, f1-score\n",
    "\n",
    "# get the list of unique labels\n",
    "labels=sorted(data[\"label\"].unique())\n",
    "\n",
    "# calculate performance metrics. \n",
    "# Support is the number of occurrences of each label\n",
    "\n",
    "precision, recall, fscore, support=\\\n",
    "     precision_recall_fscore_support(\\\n",
    "     y_test, predicted, labels=labels)\n",
    "\n",
    "print(\"labels: \", labels)\n",
    "print(\"precision: \", precision)\n",
    "print(\"recall: \", recall)\n",
    "print(\"f-score: \", fscore)\n",
    "print(\"support: \", support)\n",
    "\n",
    "# another way to get all performance metrics\n",
    "print(classification_report\\\n",
    "      (y_test, predicted, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.7.  AUC \n",
    "\n",
    "from sklearn.metrics import roc_curve, auc,precision_recall_curve\n",
    "import numpy as np\n",
    "\n",
    "# We need to get probabilities as predictions\n",
    "predict_p=clf.predict_proba(X_test)\n",
    "\n",
    "# a probability is generated for each label\n",
    "labels\n",
    "predict_p[0:3]\n",
    "# Ground-truth\n",
    "y_test[0:3]\n",
    "\n",
    "# let's just look at one label \"soc.religion.christian\"\n",
    "# convert to binary\n",
    "binary_y = np.where(y_test==\"soc.religion.christian\",1,0)\n",
    "\n",
    "# this label corresponds to last column\n",
    "y_pred = predict_p[:,3]\n",
    "\n",
    "# compute fpr/tpr by different thresholds\n",
    "# positive class has label \"1\"\n",
    "fpr, tpr, thresholds = roc_curve(binary_y, y_pred, \\\n",
    "                                 pos_label=1)\n",
    "# calculate auc\n",
    "print(\"AUC: {:.2%}\".format(auc(fpr, tpr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.figure();\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2);\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--');\n",
    "plt.xlim([0.0, 1.0]);\n",
    "plt.ylim([0.0, 1.05]);\n",
    "plt.xlabel('False Positive Rate');\n",
    "plt.ylabel('True Positive Rate');\n",
    "plt.title('AUC of Naive Bayes Model');\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.8.  precision_recall_curve\n",
    "\n",
    "# compute precision/recall by different thresholds\n",
    "precision, recall, thresholds = precision_recall_curve(binary_y, \\\n",
    "                                y_pred, pos_label=1)\n",
    "\n",
    "plt.figure();\n",
    "plt.plot(recall, precision, color='darkorange', lw=2);\n",
    "plt.xlim([0.0, 1.0]);\n",
    "plt.ylim([0.0, 1.05]);\n",
    "plt.xlabel('Recall');\n",
    "plt.ylabel('Precision');\n",
    "plt.title('Precision_Recall_Curve of Naive Bayes Model');\n",
    "plt.show();\n",
    "\n",
    "# Calculate area under PRC curver (a.k.a average precision)\n",
    "# calculate auc\n",
    "print(\"Average Precision: {:.2%}\".format(auc(recall, precision)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.9.  predict new documents\n",
    "\n",
    "docs_new = ['God is love', 'OpenGL on the GPU is fast']\n",
    "\n",
    "# generate tifid for new documents\n",
    "# note we use \"transform\" not \"fit_transform\"\n",
    "# transform creates tfidf vectors based on the\n",
    "# vocabulary established by \"fit_transform\" in Exercise 3.2.\n",
    "X_new_tfidf = tfidf_vect.transform(docs_new)\n",
    "\n",
    "print(\"new sample tf_idf size:\", X_new_tfidf.shape)\n",
    "\n",
    "# prediction\n",
    "clf.predict(X_new_tfidf)\n",
    "clf.predict_proba(X_new_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.10. Classification with stop words removed\n",
    "# Can removing stop words improves performance?\n",
    "# In Exercise 3.2, uncomment line 10 and comment line 7\n",
    "# Run Exercise 3.2, 3.5-3.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.10. Run 5-fold cross validation\n",
    "# to show the generalizability of the model\n",
    "\n",
    "# import cross validation method\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "metrics = ['precision_macro', 'recall_macro', \\\n",
    "           \"f1_macro\"]\n",
    "\n",
    "clf = MultinomialNB()\n",
    "#clf = MultinomialNB(alpha=0.5)\n",
    "\n",
    "cv = cross_validate(clf, dtm, data[\"label\"], \\\n",
    "                    scoring=metrics, cv=5, \\\n",
    "                    return_train_score=True)\n",
    "print(\"Test data set average precision:\")\n",
    "print(cv['test_precision_macro'])\n",
    "print(\"\\nTest data set average recall:\")\n",
    "print(cv['test_recall_macro'])\n",
    "print(\"\\nTest data set average f1 score:\")\n",
    "print(cv['test_f1_macro'])\n",
    "\n",
    "# To see the performance of training data set use \n",
    "# cv['train_xx_macro']\n",
    "print(\"\\nTraining data average f1 score:\")\n",
    "print(cv['train_f1_macro'])\n",
    "\n",
    "# The metrics are quite stable across folds.\n",
    "# The performance gap between training and test sets is small\n",
    "# This indicates the model has good generalizability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.11. Multinominal NB \n",
    "# with different smoothing parameter alpha\n",
    "# comment line 11 and uncomment 12 in Exercise 3.8\n",
    "# use different alpha value to see if it affects performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Exercise 3.12. SVM model\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "#from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn import svm\n",
    "\n",
    "metrics = ['precision_macro', 'recall_macro', \"f1_macro\"]\n",
    "\n",
    "# initiate an linear SVM model\n",
    "clf = svm.LinearSVC()\n",
    "\n",
    "cv = cross_validate(clf, dtm, data[\"label\"], \\\n",
    "                    scoring=metrics, cv=5)\n",
    "print(\"Test data set average precision:\")\n",
    "print(cv['test_precision_macro'])\n",
    "print(\"\\nTest data set average recall:\")\n",
    "print(cv['test_recall_macro'])\n",
    "print(\"\\nTest data set average fscore:\")\n",
    "print(cv['test_f1_macro'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Parameter tuning using grid search\n",
    "* Each classification model has a few parameters\n",
    "  * e.g. \"stop_words\": \"english\" or None, min_df: [1,2,3, ...]\n",
    "  * e.g. MultinomialNB(alpha=1.0)\n",
    "  * e.g. LinearSVC(C=1.0, penalty=’l2’, loss=’squared_hinge’,...)\n",
    "* Instead of tweaking the parameters of the various components, it is possible to run an exhaustive search of the best parameters on a grid of possible values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.3.1 Grid search\n",
    "\n",
    "# import pipeline class\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# import GridSearch\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# build a pipeline which does two steps all together:\n",
    "# (1) generate tfidf, and (2) train classifier\n",
    "# each step is named, i.e. \"tfidf\", \"clf\"\n",
    "\n",
    "text_clf = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('clf', svm.LinearSVC())\n",
    "                   ])\n",
    "\n",
    "# set the range of parameters to be tuned\n",
    "# each parameter is defined as \n",
    "# <step name>__<parameter name in step>\n",
    "# e.g. min_df is a parameter of TfidfVectorizer()\n",
    "# \"tfidf\" is the name for TfidfVectorizer()\n",
    "# therefore, 'tfidf__min_df' is the parameter in grid search\n",
    "\n",
    "parameters = {'tfidf__min_df':[1,3],\n",
    "              'tfidf__stop_words':[None,\"english\"],\n",
    "              'clf__C': [0.5,1.0,5.0],\n",
    "}\n",
    "\n",
    "# the metric used to select the best parameters\n",
    "metric =  \"f1_macro\"\n",
    "\n",
    "# GridSearch also uses cross validation\n",
    "gs_clf = GridSearchCV\\\n",
    "(text_clf, param_grid=parameters, \\\n",
    " scoring=metric, cv=5)\n",
    "\n",
    "# due to data volume and large parameter combinations\n",
    "# it may take long time to search for optimal parameter combination\n",
    "# you can use a subset of data to test\n",
    "gs_clf = gs_clf.fit(data[\"text\"], data[\"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gs_clf.best_params_ returns a dictionary \n",
    "# with parameter and its best value as an entry\n",
    "\n",
    "for param_name in gs_clf.best_params_:\n",
    "    print(\"{0}:\\t{1}\".format(param_name,\\\n",
    "                                 gs_clf.best_params_[param_name]))\n",
    "\n",
    "print(\"best f1 score: {:.3f}\".format(gs_clf.best_score_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.3.2 Grid search\n",
    "# Modify Exercise 3.3 and Exercise 3.8 \n",
    "# to use the best parameters found\n",
    "# re-create the Multinominal NB classifier\n",
    "\n",
    "# also, when setting min_df to 2, check the size of  \n",
    "# your tf-idf feature matrix. \n",
    "# How much of the dimension is reduced?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-label classification\n",
    "- So far we only cover single-label classification, i.e. assign one class to each sample\n",
    "- Multilabel classification emerges as a challenging problem, where classes are not mutually exclusive \n",
    "  * music categorization \n",
    "  * semantic classification of images\n",
    "  * tagging\n",
    "- **One-Vs-the-Rest** Strategy (a.k.a **one-vs-all**)\n",
    "  * fitting one classifier per class. For each classifier, the class is fitted against all the other classes.\n",
    "  * for $n$ classes (labels), $n$ classifier is needed\n",
    "  * Advantage: good interpretability - Since each class is represented by one and only one classifier, it is possible to gain knowledge about the class by inspecting its corresponding classifier\n",
    "  * Disadvantage: \n",
    "     * many classifiers are created if there is a large number classes\n",
    "     * ignore the structure (or dependencies) of classes\n",
    "- **Class indication matrix** (or **one-hot encoding**): Encode categorical integer features using a one-hot aka one-of-K scheme. \n",
    "\n",
    "| Document    | Money       | Investment | Crime & Justice |\n",
    "| :-----------|:-----------:|:----------:|:--------------:|\n",
    "| 1           | 0           |      0     | 1              |\n",
    "| 2           | 1           |      1     | 0              |\n",
    "| 3           | 1           |      0     | 0              |\n",
    "| 4           | 0           |      1     | 1              |\n",
    "\n",
    "- **dataset**: Yahoo News Ranked Multilabel Learning dataset (http://research.yahoo.com)\n",
    "  - A subset is selected\n",
    "  - 4 classes, 6426 samples\n",
    "  \n",
    "- **Discussion**: can you apply Naive Bayes for multi-label classification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.1 Multi-label classification- Load data\n",
    "\n",
    "import json\n",
    "data=json.load(open(\"../../dataset/ydata.json\",\"r\"))\n",
    "\n",
    "docs,labels=zip(*data)\n",
    "\n",
    "# show sample examples\n",
    "docs[1]\n",
    "labels[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.2 One-hot coding of classes\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import numpy as np\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "Y=mlb.fit_transform(labels)\n",
    "# check size of indicator matrix\n",
    "# print some rows \n",
    "Y[0:5]\n",
    "Y.shape\n",
    "# check classes\n",
    "mlb.classes_\n",
    "\n",
    "# check # of samples in each class\n",
    "np.sum(Y, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exercise 4.3 Multi-label classification- one vs. rest classifier\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# split dataset into train (70%) and test sets (30%)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\\\n",
    "                docs, Y, test_size=0.3, random_state=0)\n",
    "\n",
    "classifier = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words=\"english\",\\\n",
    "                              min_df=2)),\n",
    "    ('clf', OneVsRestClassifier(LinearSVC()))])\n",
    "\n",
    "classifier.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.4 Multi-label classification- Performance report\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "predicted = classifier.predict(X_test)\n",
    "\n",
    "predicted.shape\n",
    "print(\"predicted:\")\n",
    "predicted[0:2]\n",
    "print(\"actual:\")\n",
    "Y_test[0:2]\n",
    "\n",
    "print(classification_report\\\n",
    "      (Y_test, predicted, target_names=mlb.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/harshitahiremath/BIA 660-C : Web Mining\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
