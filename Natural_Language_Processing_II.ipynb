{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Natural Language Processing Using NLTK (II)</center>\n",
    "\n",
    "References:\n",
    " - http://www.nltk.org/book_1ed/\n",
    " - https://web.stanford.edu/class/cs124/lec/Information_Extraction_and_Named_Entity_Recognition.pdf\n",
    " - https://nlpforhackers.io/complete-guide-to-spacy/\n",
    " - https://radimrehurek.com/gensim/models/phrases.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. NLP Objectives and Basic Steps\n",
    "\n",
    " - Objectives:\n",
    "   * Split documents into words, punctuation sysmbols, or segments\n",
    "   * Understand vocabulary of the text\n",
    "   * Extract features for further text mining tasks\n",
    " - Basic processing steps:\n",
    "   * Tokenization: split documents into individual words and punctuation symbols\n",
    "   * Remove stop words and filter tokens\n",
    "   * **POS (part of speech) Tagging**  \n",
    "   * **Normalization: Stemming, Lemmatization**\n",
    "   * **Named Entity Recognition (NER)**\n",
    "   * **Term Frequency and Inverse Dcoument Frequency (TF-IDF)**\n",
    "   * **Create document-to-term matrix (bag of words)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Sample text for analysis\n",
    "\n",
    "news=[\"Oil prices soar to all-time record\", \n",
    "\"Stocks end up near year end\", \n",
    "\"Money funds rose in latest week\",\n",
    "\"Stocks up; traders eye crude oil prices\",\n",
    "\"Dollar rising broadly on record trade gain\"]\n",
    "text=\". \".join(news).lower()\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. POS (Part of Speech) Tagging\n",
    "\n",
    " - What is POS Tagging:\n",
    "   * The process of marking up a word in a text as corresponding to a particular part of speech (e.g. nouns, verbs, adjectives, adverbs etc.), based on both **its definition**, as well as its **context** — adjacent and related words in a phrase, sentence, or paragraph. \n",
    " - Why POS Tagging: \n",
    "   * **disambiguation**: A word may have different meanings. POS tag is a potential strong signal for word sense disambiguation. For example, \"I fish a fish\"\n",
    "   * **Phrase extraction**: Use POS rules to define accepted phrases (or information unit), or collocations for indexing and retrieval:\n",
    "     * Adj + Noun, e.g. nice house\n",
    "     * Verb + Noun, e.g. play football\n",
    "     * typical collocation patterns (https://nlp.stanford.edu/fsnlp/promo/colloc.pdf):\n",
    "       - Adj + Noun: e.g. linear function\n",
    "       - Noun + Noun: e.g. regression coefficient\n",
    "       - Adj + Adj + Noun: e.g. Gaussian random variable\n",
    "       - Noun + Adj + Noun: e.g. mean squared error\n",
    "       - Noun + Noun + Noun: e.g. class probability function\n",
    "       - Noun + Preposition + Noun: e.g. dregrees of freedom\n",
    "   * **Filter tokens**:  some POS have less importance in retrieval, e.g. stopwords such as ‘a’, ‘an’, ‘the’, and other glue words like 'in', 'on', 'of' etc.\n",
    "   * Find other forms of a word based on POS\n",
    "        * Noun: plural and singular\n",
    "        * Verb: past, present and future tense\n",
    "        * Adjective: positive, comparative, and superlative\n",
    " - List of Penn Treebank Tags can be found at https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    " - A tagger (program for tagging) is trained based on a corpus using machine learning approaches. It may not be very accurate when applying it your corpus.\n",
    "   - Stanford tagger (~97%)\n",
    "   - NLTK default tagger (PerceptronTagger)\n",
    "- How to train a tagger?\n",
    "    - *Stochastic/Probabilistic Methods*: Automated ways to assign a PoS to a word based on the probability that a word belongs to a particular tag or based on the probability of a word being a tag based on a sequence of preceding/succeeding words.\n",
    "    - Algorithms: SVM, Naive Bayes, CRF (conditional random fields), Hidden Markov Models (HMM)\n",
    "    - Features: \n",
    "        - **Word**: the word itself. Some words are always one PoS, others not.\n",
    "        - **is_first**, is_last: check if it is the first or last in the sentence.\n",
    "        - **is_capitalized**: first letter is caps? Maybe it is a proper noun...\n",
    "        - **is_all_caps** or *is_all_lower*: checks for acronyms (or common words).\n",
    "        - **prefixes/suffixes**: check word initialization/termination\n",
    "        - **prev_word/next_word**: checks the preceding and succeding word.\n",
    "        - **has-hyphen**: words with '-' may be adjectives.\n",
    "        - **is_numeric**: for numbers.\n",
    "        - **capitals_inside**: weird cases. Maybe nouns.\n",
    "   - Check https://colab.research.google.com/drive/1d7LO_0665DYw6DrVJXXautJAJzHHqYOm#scrollTo=bfD5ujGijuUF for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.1. To find all tags in treebank\n",
    "#nltk.help.upenn_tagset()\n",
    "\n",
    "# find the meaning of a specific tag\n",
    "nltk.help.upenn_tagset('JJ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.2. NLTK POS Tagging\n",
    "\n",
    "# The input to the tagging function is a list of words\n",
    "\n",
    "# tokenize the text\n",
    "tokens=nltk.word_tokenize(text)\n",
    "\n",
    "# tag each tokenized word\n",
    "tagged_tokens= nltk.pos_tag(tokens)\n",
    "\n",
    "tagged_tokens\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.3. Extract Phrases by POS\n",
    "\n",
    "# Extract phrases in pattern of adjective + noun\n",
    "# i.e. nice house, growing market\n",
    "\n",
    "bigrams=list(nltk.bigrams(tagged_tokens))\n",
    "#print(bigrams)\n",
    "\n",
    "phrases=[ (x[0],y[0]) for (x,y) in bigrams \\\n",
    "         if x[1].startswith('JJ') \\\n",
    "         and y[1].startswith('NN')]\n",
    "\n",
    "print(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.4. Extract Noun+Verb, \n",
    "# i.e. prices soar\n",
    "\n",
    "phrases=[ (x[0],y[0]) for (x,y) in bigrams \\\n",
    "         if x[1].startswith('NN') \\\n",
    "         and y[1].startswith('VB')]\n",
    "\n",
    "print(phrases)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Normalization: Stemming & Lemmatization\n",
    " - What is normalization:\n",
    "   - Converts a list of words in **different surface forms** to a more **uniform form**, e.g.\n",
    "        * a word with different forms, e.g. organize, organizes, organized, and organizing\n",
    "        * families of derivationally related words with similar meanings, such as democracy, democratic, and democratization.\n",
    " - Why normalization\n",
    "   - **improve text matching**: in many situations, it seems as if it would be useful for a search for one of these words to return documents that contain another word in the set.\n",
    "   - reduce featue space generated from text\n",
    " - Stemming and lemmatization are two common techinques\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Stemming \n",
    "\n",
    "* **Stemming**: reducing inflected (or sometimes derived) words to their **stem, base or root** form. \n",
    "   * For example, **crying** -> **cri**. \n",
    "   * Stemming may not generate a real word, but a root form. \n",
    "   * The stemming program is called stemmer. \n",
    "       * Famous stemers are Porter stemmer, Lancaster Stemmer, Snowball Stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5.1.1. Stermming Using Porter Stemmer\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "print(\"Stem of organizing/organized/organizes/organization\")\n",
    "print(porter_stemmer.stem('organizing'))\n",
    "print(porter_stemmer.stem('organized'))\n",
    "print(porter_stemmer.stem('organizes'))\n",
    "print(porter_stemmer.stem('organization'))\n",
    "\n",
    "print(\"\\nStem of crying\")\n",
    "print(porter_stemmer.stem('crying'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Lemmatization\n",
    "\n",
    "* **Lemmatization**: determining the lemma for a given word, \n",
    "   * A lemma is a word which stands at the head of a definition in a dictionary, e.g. run (lemma),  runs, ran and running (inflections) \n",
    "   * Lemmatization is a complex task involving understanding context and determining the part of speech of a word in a sentence \n",
    "      * e.g. \"organized\" (verb or adjective?)\n",
    "   * The widely used Lemmatization method is based on WordNet, a large lexical database of English.\n",
    "\n",
    "* **Difference** between stemming and lemmatization: \n",
    "   * a stemmer operates on a single word **without knowledge of the context**, and therefore cannot discriminate between words which have different meanings depending on part of speech. While, lemmatization **requires context and POS tags**. \n",
    "   * Stemming may not generate a real word, but lemmization always generates real words.\n",
    "   *  However, stemmers are typically easier to implement and run faster with reduced accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5.2.1. Lemmatization\n",
    "\n",
    "# wordnet lemmatizer takes POS tag as a parameter\n",
    "# However, wordnet has its own tag set, \n",
    "# different from treebank tag set\n",
    "# The default POS tag is noun \n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(\"organizing (verb) ->\", \\\n",
    "      wordnet_lemmatizer.lemmatize\\\n",
    "      ('organizing', wordnet.VERB))\n",
    "print('organized (verb) ->', \\\n",
    "      wordnet_lemmatizer.lemmatize\\\n",
    "      ('organized', wordnet.VERB))\n",
    "print('organized (adjective) ->',\\\n",
    "      wordnet_lemmatizer.lemmatize('organized', \\\n",
    "                                   wordnet.ADJ))\n",
    "print('organization (noun) ->',\\\n",
    "      wordnet_lemmatizer.lemmatize('organization'))\n",
    "print('crying (adjective) ->',\\\n",
    "      wordnet_lemmatizer.lemmatize('crying', \\\n",
    "                                   wordnet.ADJ))\n",
    "print('crying (verb) ->', \\\n",
    "      wordnet_lemmatizer.lemmatize('crying', \\\n",
    "                                   wordnet.VERB))\n",
    "\n",
    "# compare the result with Exercise 5.1.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Named Entity Recognition (NER)\n",
    "\n",
    "- Definition: find and classify real word entities (Person, Organization, Event etc.) in text\n",
    "- Example: sentence \"Jim bought 300 shares of Acme Corp. in 2006\" can be annotated as \"**[Jim]<sub>Person</sub>** bought 300 shares of **[Acme Corp.]<sub>Organization</sub>** in 2006\"\n",
    "- Uses of NER:\n",
    "   *  Information Extraction: extract clear, factual information, i.e., Who did what to whom when?\n",
    "   *  Named entities can be indexed, and their relations can be extracted.\n",
    "   *  Sentiment can be attributed to companies or products\n",
    "   *  For question answering, answers are often named entities.\n",
    "- Techniques for NER\n",
    "   * Regular expression: Telephone numbers, emails, Capital names (e.g. Capitalized word + {city,  center, river}\n",
    "      * Adantages: simple and sometimes effective\n",
    "      * Disadvantage: \n",
    "         * first word of a sentence is capitalized; sometimes, titles are all capitalized; new proper names constantly emerges (e.g. movie titles, books, etc.)\n",
    "         * proper names may be ambiguous, e.g. Jordan can be *person* or *location*\n",
    "   * Supervised learning (IOB) (https://arxiv.org/abs/cmp-lg/9505040)\n",
    "       1. Collect a set of representative training documents\n",
    "       2. Label each token for its entity class (I: inside entity, B: begining entity) or other (O)\n",
    "       3. Design feature extractors appropriate to the text and classes, e.g. current word, pre/next word, pos tags etc.\n",
    "       4. Train a sequence classifier to predict the labels from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 6.1. Use NLTK for Named Entity Recognition\n",
    "\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk, Tree\n",
    "\n",
    "sentence = \"Jim bought 300 shares of Acme Corp. in 2006.\"\n",
    "\n",
    "# the input to ne_chunk is list of (token, pos tag) tuples\n",
    "ner_tree=ne_chunk(pos_tag(word_tokenize(sentence)))\n",
    "\n",
    "# ne_chunk returns a tree\n",
    "# print the tree\n",
    "Tree.fromstring(str(ner_tree)).pretty_print()\n",
    "\n",
    "\n",
    "# get PERSON out of the tree\n",
    "person=[]\n",
    "for t in ner_tree.subtrees():\n",
    "    if t.label() == 'PERSON':\n",
    "        person.append(t.leaves())\n",
    "print(\"PERSON\",person)\n",
    "\n",
    "\n",
    "# how to extract organization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Term Frequency and Inverse Dcoument Frequency (TF-IDF)\n",
    " - Motivation: How to identify important words (or phrases, named entities) in a text in a collecton or corpus? When search for documents, we'd like to have these important words are matched.\n",
    " - Intuition: \n",
    "   * In a document, if a word/term/phrase is repeated many times, it is likely important. \n",
    "   * However, if it appears in most of the documents in the corpus, then it has little discriminating power in determining relevance. \n",
    "   * For instance, a collection of documents on the auto industry is likely to have the term auto in almost every document. Search by \"auto\" you may get all the documents. \n",
    " - **TF-IDF**: is composed by two terms: \n",
    "      - `TF (Term Frequency)`: which measures how frequently a term, say w, occurs in a document. \n",
    "      - `IDF (Inverse Document Frequency)`: measures how important a term is within the corpus. \n",
    " \n",
    " - TF-IDF provides another way to remove stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. Term Frequency (TF)\n",
    "- Measures how frequently a term, say w, occurs in a document, say $d$. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. \n",
    "- Thus, the frequency of $w$ in $d$, denoted as $freq(w,d)$ is often divided by the document length (a.k.a. the total number of terms in the document, denoted as $|d|$) as a way of normalization: $$tf(w,d) = \\frac{freq(w,d)}{|d|}$$\n",
    "- Example: d=\"Stocks end up near year end\"\n",
    "   * `tf('Stocks',d)=?`\n",
    "   * `tf('end',d)=?`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. Inverse Document Frequency (IDF)\n",
    "- Measures how important a term is within the corpus. \n",
    "- However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. \n",
    "- Thus we need to weigh down the frequent terms while scale up the rare ones. \n",
    "- Let $|D|$ denote the number of documents, $df(w,D)$ denotes the number of documents with term $w$ in them. Then, $$idf(w) = ln(\\frac{|D|}{df(w,D)})+1$$ Or a smoothed version: $$idf(w) = ln(\\frac{|D|+1}{df(w,D)+1})+1$$\n",
    "- Examples: \n",
    "  * Considering dataset:\n",
    "       1. \"Oil prices soar to all-time record\", \n",
    "       2. \"Stocks end up near year end\", \n",
    "       3. \"Money funds rose in latest week\",\n",
    "       4. \"Stocks up; traders eye crude oil prices\",\n",
    "       5. \"Dollar rising broadly on record trade gain\"\n",
    "  * `idf('Stocks')=`?\n",
    "  * `idf('all-time')=`?\n",
    "  * Discussion:\n",
    "     * What words get very low IDF score?\n",
    "     * What words get very high IDF score?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3. TF-IDF \n",
    "- Let $s(w,d)=tf(w,d) * idf(w)$, normalize the TF-IDF score of each word in a document normalized by the Euclidean norm, then \n",
    "   $$tfidf(w,d)=\\frac{s(w,d)}{\\sqrt{\\sum_{w \\in d}{s(w,d)^2}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 7.1. computing tf-idf\n",
    "\n",
    "\n",
    "import nltk, re, string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# library for normalization\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# numpy is the package for matrix caculation\n",
    "import numpy as np  \n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "docs=[\"Oil prices soar to all-time record\", \n",
    "\"Stocks end up near year end\", \n",
    "\"Money funds rose in latest week\",\n",
    "\"Stocks up; traders eye crude oil prices\",\n",
    "\"Dollar rising broadly on record trade gain\"]   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1. get tokens of each document as list\n",
    "\n",
    "def get_doc_tokens(doc):\n",
    "    tokens=[token.strip() \\\n",
    "            for token in nltk.word_tokenize(doc.lower()) \\\n",
    "            if token.strip() not in stop_words and\\\n",
    "               token.strip() not in string.punctuation]\n",
    "    \n",
    "    # you can add bigrams, collocations, or lemmatization here\n",
    "    \n",
    "    # create token count dictionary\n",
    "    token_count=nltk.FreqDist(tokens)\n",
    "    \n",
    "    # or you can create dictionary by yourself\n",
    "    #token_count={token:tokens.count(token) for token in set(tokens)}\n",
    "    return token_count\n",
    "\n",
    "# step 2. process all documents to \n",
    "# a dictionary of dictionaries\n",
    "docs_tokens={idx:get_doc_tokens(doc) \\\n",
    "             for idx,doc in enumerate(docs)}\n",
    "docs_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3. get document-term matrix\n",
    "# contruct a document-term matrix where \n",
    "# each row is a doc \n",
    "# each column is a token\n",
    "# and the value is the frequency of the token\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# since we have a small corpus, we can use dataframe \n",
    "# to get document-term matrix\n",
    "# but don't use this when you have a large corpus\n",
    "\n",
    "dtm=pd.DataFrame.from_dict(docs_tokens, \\\n",
    "                           orient=\"index\" )\n",
    "dtm\n",
    "dtm=dtm.fillna(0)\n",
    "dtm\n",
    "\n",
    "# sort by index (i.e. doc id)\n",
    "dtm = dtm.sort_index(axis = 0)\n",
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4. get normalized term frequency (tf) matrix\n",
    "\n",
    "# convert dtm to numpy arrays\n",
    "tf=dtm.values\n",
    "\n",
    "# sum the value of each row\n",
    "doc_len=tf.sum(axis=1)\n",
    "doc_len\n",
    "\n",
    "# divide dtm matrix by the doc length matrix\n",
    "tf=np.divide(tf, doc_len[:,None])\n",
    "\n",
    "# set float precision to print nicely\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 5. get idf\n",
    "\n",
    "# get document freqent\n",
    "df=np.where(tf>0,1,0)\n",
    "#df\n",
    "\n",
    "# get idf\n",
    "idf=np.log(np.divide(len(docs), \\\n",
    "        np.sum(df, axis=0)))+1\n",
    "print(\"\\nIDF Matrix\")\n",
    "idf\n",
    "\n",
    "# what is the size of idf array?\n",
    "\n",
    "smoothed_idf=np.log(np.divide(len(docs)+1, np.sum(df, axis=0)+1))+1\n",
    "print(\"\\nSmoothed IDF Matrix\")\n",
    "smoothed_idf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 6. get tf-idf\n",
    "print(\"TF-IDF Matrix\")\n",
    "s = tf*idf\n",
    "s\n",
    "\n",
    "tf_idf=normalize(tf*idf)   # is broadcast possible here?\n",
    "tf_idf\n",
    "\n",
    "print(\"\\nSmoothed TF-IDF Matrix\")\n",
    "smoothed_tf_idf=normalize(tf*smoothed_idf)\n",
    "smoothed_tf_idf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TF-IDF matrix gives **weight** of each word in each document\n",
    "- Documents:\n",
    "    1. \"Oil prices soar to all-time record\", \n",
    "    2. \"Stocks end up near year end\", \n",
    "    3. \"Money funds rose in latest week\",\n",
    "    4. \"Stocks up; traders eye crude oil prices\",\n",
    "    5. \"Dollar rising broadly on record trade gain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For better visualization, let's make the tf-idf array a dataframe\n",
    "pd.options.display.float_format = '{:,.2f}'.format # set format for float\n",
    "\n",
    "pd.DataFrame(smoothed_tf_idf, columns = dtm.columns)\n",
    "# the dtm dataframe we created in Step 3 has each word as a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 7.2. Find the top three words \n",
    "# of each document by TF-IDF weight\n",
    "\n",
    "top=smoothed_tf_idf.argsort(axis = 1)[:,::-1][:,0:3]\n",
    "top\n",
    "\n",
    "for row in top:\n",
    "    print([dtm.columns[x] for x in row])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4. What to do with TF-IDF\n",
    "- This is the `feature sapce` of text mining (a.k.a. `Bag of Words` or `Vector Space Model`)\n",
    "- Identify important words in each document\n",
    "- Find similar documents\n",
    "    * How to measure simialrity (or distance)? http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.332.4480&rep=rep1&type=pdf\n",
    "        - `Euclidean distance`\n",
    "        - `Cosine distance`\n",
    "    * Euclidean distance:\n",
    "        - It can be **large** for vectors of high dimension \n",
    "        - `Curse of dimensionality`: In a high-dimensional space, the ratio between the nearest and farthest points approaches 1, i.e. the points essentially become uniformly distant from each other. (https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf)\n",
    "    * Cosine similarity: The similarity between two documents is a function of the angle between their vectors in the if-idf vector space. \n",
    "      <img src='cosine.png' width=50% />\n",
    "      <img src='cosine_formula.svg' width=50% />\n",
    "      - Example: A=[0,2,1], B=[1,1,2], then\n",
    "      $$cosine(A,B)=\\frac{0*1+2*1+1*2}{\\sqrt{0+4+1}*\\sqrt{1+1+4}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exercise 7.4.1 Document similarity\n",
    "\n",
    "# package to calculate distance\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "# calculate cosince distance of every pair of documents \n",
    "# similarity is 1-distance\n",
    "similarity=1-pairwise_distances(tf_idf, metric = 'cosine')\n",
    "similarity\n",
    "\n",
    "# find top doc similar to the first one\n",
    "# Note the diagonal value is 1, which is the largest\n",
    "\n",
    "np.argsort(similarity)[:,::-1][0,0:2]\n",
    "\n",
    "for idx, doc in enumerate(docs):\n",
    "    print(idx,doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5. Put Everyting together -- Computing TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, string\n",
    "from sklearn.preprocessing import normalize\n",
    "from nltk.corpus import stopwords\n",
    "# numpy is the package for matrix cacluation\n",
    "import numpy as np  \n",
    "import pandas as pd\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Step 1. get tokens of each document as list\n",
    "def get_doc_tokens(doc):\n",
    "    tokens=[token.strip() \\\n",
    "            for token in nltk.word_tokenize(doc.lower()) \\\n",
    "            if token.strip() not in stop_words and\\\n",
    "               token.strip() not in string.punctuation]\n",
    "    \n",
    "    # you can add bigrams, collocations, stemming, \n",
    "    # or lemmatization here\n",
    "    \n",
    "    token_count={token:tokens.count(token) for token in set(tokens)}\n",
    "    return token_count\n",
    "\n",
    "def tfidf(docs):\n",
    "    # step 2. process all documents to get list of token list\n",
    "    docs_tokens={idx:get_doc_tokens(doc) \\\n",
    "             for idx,doc in enumerate(docs)}\n",
    "\n",
    "    # step 3. get document-term matrix\n",
    "    dtm=pd.DataFrame.from_dict(docs_tokens, orient=\"index\" )\n",
    "    dtm=dtm.fillna(0)\n",
    "    dtm = dtm.sort_index(axis = 0)\n",
    "      \n",
    "    # step 4. get normalized term frequency (tf) matrix        \n",
    "    tf=dtm.values\n",
    "    doc_len=tf.sum(axis=1, keepdims=True)\n",
    "    tf=np.divide(tf, doc_len)\n",
    "    \n",
    "    # step 5. get idf\n",
    "    df=np.where(tf>0,1,0)\n",
    "    #idf=np.log(np.divide(len(docs), \\\n",
    "    #    np.sum(df, axis=0)))+1\n",
    "\n",
    "    smoothed_idf=np.log(np.divide(len(docs)+1, np.sum(df, axis=0)+1))+1    \n",
    "    smoothed_tf_idf=tf*smoothed_idf\n",
    "    \n",
    "    return smoothed_tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Keyowrd/Keyphrase Extraction\n",
    "\n",
    "Why keywords/keyphrses (Wang and Xiao 2008)\n",
    "- A keyphrase is defined as a meaningful and significant expression consisting of one or more words in a document. \n",
    "- Appropriate keyphrases can serve as a highly condensed summary for a document, and they can be used as a label for the document to supplement or replace the title or summary, or they can be highlighted within the body of the document to facilitate users’ fast browsing and reading. \n",
    "- Moreover, document keyphrases have been successfully used in the following IR and NLP tasks: document indexing, document classification, document clustering, and document summarization.\n",
    "\n",
    "References: \n",
    "- Mihalcea, R., & Tarau, P. (2004, July). TextRank: Bringing order into texts. Association for Computational Linguistics.\n",
    "- Wan, Xiaojun and Jianguo Xiao. 2008. Single document keyphrase extraction using neighborhood knowledge. In Proceedings of the 23rd AAAI Conference on Artificial Intelligence, pages 855–860.\n",
    "- Florescu, C. and Cornelia, C. (2017). PositionRank: An Unsupervised Approach to Keyphrase Extraction from Scholarly Documents. In proceedings of ACL*, pages 1105-1115\n",
    "\n",
    "A nice tutorial: <a href='https://towardsdatascience.com/textrank-for-keyword-extraction-by-python-c0bae21bcec0'> Understand TextRank for Keyword Extraction by Python </a>\n",
    "   - Concept Map example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://miro.medium.com/max/1400/1*JeYlqJTA5MpKsaWwHkhKQw.jpeg' width='70%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Textrank\n",
    "\n",
    "TextRank is an algorithm based on PageRank, which often used in keyword extraction and text summarization. \n",
    "\n",
    "**Pagerank** \n",
    "\n",
    "- PageRank is used primarily for ranking web pages in online search results \n",
    "- All web pages as a big directed graph, where a node is a webpage and each edge is a link from the source page to the target page. \n",
    "- For example, if webpage A has the link to web page B, it can be represented as a directed edge from A to B.\n",
    "- <a href= \"https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/\"> An illustrated graph </a>\n",
    "<img src = 'https://cdn.analyticsvidhya.com/wp-content/uploads/2018/10/Pagerank11.png' width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pagerank formula\n",
    "\n",
    "    - <img src='https://miro.medium.com/max/1400/1*hheHfLOTjPW3uSsSxWKylQ.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Example\n",
    "<img src ='https://miro.medium.com/max/1030/1*DkZjlRNEjPSc8RNL7yWggA.jpeg' width = '60%'>\n",
    "    \n",
    "    \n",
    "Initial transition matrix: each column indicates the transition probability from one node to others, e.g. p(e|a) = 1.\n",
    "\n",
    "\n",
    "Let's focus on nodes `a, b, e, f`. The transition matrix is shown below:    \n",
    "<img src='https://miro.medium.com/max/1400/1*ppIReXWpavbuvAHfi08EGw.png' width=\"80%\">\n",
    "\n",
    "\n",
    "Assuming initial weight `V` for each node is 1 and `d = 0.85`. We can update `V` iteratively until `V` converges.\n",
    "- For example, $V(e) = (1-0.85) + 0.85*(1 + \\frac{1}{2} * 1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Textrank: rank words within a document\n",
    "\n",
    "- We consider text within a specific window as a chunk\n",
    "- Formulate each word as a node\n",
    "- Chunks are used as edges to connect nodes. The weight on the edge between each pair of nodes denotes the number of chunks sharing the same pair of words\n",
    "- Then we follow pagerank to rank the importance of each node\n",
    "\n",
    "- Example sentence:  `The Wandering Earth feels like a throwback to a few familiar eras of American filmmaking.`\n",
    "    - After removing punctuations and stopping words, we'll have `[Wandering, Earth, feels, throwback, familiar, eras, American, filmmaking]`\n",
    "    - Consider window = 3, we'll have chunks `[Wandering, Earth, feels], [Earth, feels, throwback], ...`\n",
    "    - Formulate the graph as follows:\n",
    "        - Each word is a node\n",
    "        - The edge weight between two nodes is the times they show in trunks (edge weight can also be binarized). \n",
    "    - Calculate textrank for each word\n",
    "    - Rank each word by their text rank\n",
    "\n",
    "\n",
    "\n",
    "- A reference implementation (shown below) can be found at https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/.\n",
    "- This algorithm is also implemented at `Textacy` package: https://textacy.readthedocs.io/en/0.11.0/api_reference/extract.html#module-textacy.extract.keyterms.textrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
