{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mT2Nlay6GA-a"
   },
   "source": [
    "# HW 5: Clustering and Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vgrsQkhtGA-c"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">Each assignment needs to be completed independently. Never ever copy others' work (even with minor modification, e.g. changing variable names). Anti-Plagiarism software will be used to check all submissions. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5JIbCbPGGA-d"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">If you use GPT to generate code, make sure you understand and customize the generated code. Keep in mind that it is not guaranteed that the generated code can satisfy all requirements and the code can even be executed! Also keep in mind that similar submissions are suspects of plagiarism.  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rebtr-1IGA-d"
   },
   "source": [
    "In this assignment, you'll practice different text clustering methods. For unsupervised learning, we have a training set of text, and a testing set with labels.\n",
    "\n",
    "Sample outputs have been provided to you. Due to randomness, you may not get the same result as shown here. Your taget is to achieve about 70% F1 for the test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3NPaT54-GA-e"
   },
   "source": [
    "## Q1: K-Mean Clustering\n",
    "\n",
    "Define a function `cluster_kmean(train_text, test_text, text_label)` as follows:\n",
    "- Take three inputs:\n",
    "    - `train_text` is a list of documents for traing\n",
    "    - `test_text` is a list of documents for test\n",
    "    - `test_label` is the labels corresponding to documents in `test_text`\n",
    "- First generate `TFIDF` weights. You need to decide appropriate values for parameters such as `stopwords` and `min_df`:\n",
    "    - Keep or remove stopwords? Customized stop words?\n",
    "    - Set appropriate `min_df` to filter infrequent words\n",
    "- Use `KMeans` to cluster documents in `train_text` into 4 clusters. Here you need to decide the following parameters:\n",
    "    \n",
    "    - Distance measure: `cosine similarity`  or `Euclidean distance`? Pick the one which gives you better performance.  \n",
    "    - When clustering, be sure to  use sufficient iterations with different initial centroids to make sure clustering converge.\n",
    "- Test the clustering model performance using `test_label` as follows:\n",
    "  - Predict the cluster ID for each document in `test_text`.\n",
    "  - Apply `majority vote` rule to dynamically map the predicted cluster IDs to `test_label`. Note, you'd better not hardcode the mapping, because cluster IDs may be assigned differently in each run. (hint: if you use pandas, look for `idxmax` function).\n",
    "  - print out the classification report for the test subset\n",
    "  \n",
    "  \n",
    "- This function has no return. Print out the classification report.\n",
    "\n",
    "\n",
    "- Briefly discuss the following questions.\n",
    "    - What preprocessing parameters are better and why.\n",
    "    - Which distance measure is better and why it is better.\n",
    "    - Could you assign a meaningful name to each cluster? Discuss how you interpret each cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "71WOI51vGA-e"
   },
   "outputs": [],
   "source": [
    "# Add your import statement\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "# add import statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p-PuXZDrGA-f",
    "outputId": "c7a2cab1-923e-4b44-9668-bf4d9e54f3bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      The Academy of Motion Picture Arts and Science...\n",
       "1      Jim Carrey’s latest portrait is a haunting tri...\n",
       "2      Actress Ali Wentworth knows she bears a striki...\n",
       "3       \"Film Festivals are still an important outlet...\n",
       "4      In her 1940 “Self Portrait with Cropped Hair,”...\n",
       "                             ...                        \n",
       "746    As Tracey Scott Wilson's Buzzer gets underway ...\n",
       "747     The Museum. Close to celebrating its 40th ann...\n",
       "748    Becca “Do the Damn Thing” Kufrin’s season of “...\n",
       "749    Why do people leave organizations? Reasons oft...\n",
       "750    WASHINGTON ― House Republicans say they’re mak...\n",
       "Name: body, Length: 751, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"train5.csv\")\n",
    "train_text=train[\"body\"]\n",
    "\n",
    "test = pd.read_csv(\"test5.csv\")\n",
    "\n",
    "test_label = test[\"category\"]\n",
    "test_text = test[\"body\"]\n",
    "\n",
    "test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 242
    },
    "id": "IkA5oQbLGA-f",
    "outputId": "d8cd704a-a5a2-4466-904f-f124468acb6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 58409), ('to', 34058), ('of', 32158), ('and', 31154), ('a', 29508), ('in', 21868), ('that', 14991), ('is', 11713), ('for', 11085), ('on', 9798), ('with', 8841), ('I', 7802), ('as', 7440), ('The', 7429), ('was', 7308), ('it', 6030), ('are', 5736), ('by', 5683), ('at', 5591), ('be', 5271)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Tokenize the text into words\n",
    "tokens = \" \".join(train_text).split()\n",
    "\n",
    "# Calculate word frequencies\n",
    "word_frequencies = Counter(tokens)\n",
    "\n",
    "# Display the top N words by frequency\n",
    "top_words = word_frequencies.most_common(20)\n",
    "print(top_words)\n",
    "\n",
    "# Choose a threshold frequency to identify less informative words\n",
    "threshold_frequency = 500\n",
    "\n",
    "# Identify words with frequency below the threshold for exclusion\n",
    "less_informative_words = [word for word, frequency in top_words if frequency <= threshold_frequency]\n",
    "\n",
    "common_stopwords = set(ENGLISH_STOP_WORDS)\n",
    "# Add less informative words to your custom stopwords\n",
    "custom_stopwords = common_stopwords.union(less_informative_words)\n",
    "less_informative_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kE5LOHGhGA-f",
    "outputId": "c4263943-0c67-405d-d296-40c20fbd8bba"
   },
   "outputs": [],
   "source": [
    "def cluster_kmean(train_text, test_text, test_label):\n",
    "\n",
    "\n",
    "    # Add your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "iK6WAvtMGA-g"
   },
   "outputs": [],
   "source": [
    "def cluster_kmean(train_text, test_text, test_label,stop_words):\n",
    "    # TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer(stop_words = stop_words ,min_df=5)\n",
    "\n",
    "    # KMeans clustering\n",
    "    kmeans = KMeans(n_clusters=4, random_state=42, n_init=50, max_iter=1000,init='k-means++')\n",
    "\n",
    "    # Create a pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf', vectorizer),\n",
    "        ('kmeans', kmeans)\n",
    "    ])\n",
    "\n",
    "    # Fit the pipeline on the training data\n",
    "    pipeline.fit(train_text)\n",
    "\n",
    "    # Predict cluster labels for test data\n",
    "    predicted_labels = pipeline.predict(test_text)\n",
    "\n",
    "    # Map cluster IDs to labels using majority vote\n",
    "    cluster_to_label = {}\n",
    "    for cluster_id in range(4):\n",
    "        majority_label = test_label[predicted_labels == cluster_id].mode().values[0]\n",
    "        cluster_to_label[cluster_id] = majority_label\n",
    "\n",
    "    # Map predicted cluster IDs to test labels\n",
    "    mapped_labels = [cluster_to_label[cluster_id] for cluster_id in predicted_labels]\n",
    "\n",
    "    # Print classification report\n",
    "    print(classification_report(test_label, mapped_labels))\n",
    "\n",
    "    # Interpretation of clusters (you can customize these based on your data)\n",
    "    print(\"\\nCluster Interpretation:\")\n",
    "    for cluster_id in range(4):\n",
    "        cluster_text = [test_text[i] for i in range(len(test_text)) if predicted_labels[i] == cluster_id]\n",
    "        print(f\"Cluster {cluster_id + 1}: {len(cluster_text)} documents\")\n",
    "        # Additional analysis or interpretation for each cluster can be added here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2jZVBfWaGA-g",
    "outputId": "f1dfb39b-5531-42b2-b0b2-4d9ca19e414b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "ARTS & CULTURE       0.59      0.95      0.73       297\n",
      "      BUSINESS       0.51      0.72      0.60       142\n",
      " ENTERTAINMENT       0.00      0.00      0.00       168\n",
      "      POLITICS       0.68      0.35      0.47       144\n",
      "\n",
      "      accuracy                           0.58       751\n",
      "     macro avg       0.45      0.50      0.45       751\n",
      "  weighted avg       0.46      0.58      0.49       751\n",
      "\n",
      "\n",
      "Cluster Interpretation:\n",
      "Cluster 1: 15 documents\n",
      "Cluster 2: 476 documents\n",
      "Cluster 3: 75 documents\n",
      "Cluster 4: 185 documents\n"
     ]
    }
   ],
   "source": [
    "result = cluster_kmean(train_text, test_text, test_label, 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8uvqVALBGA-g",
    "outputId": "8d85cca4-3a1a-4165-bf6c-c8b371441b75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "ARTS & CULTURE       0.82      0.66      0.73       297\n",
      "      BUSINESS       0.52      0.24      0.33       142\n",
      " ENTERTAINMENT       0.59      0.81      0.69       168\n",
      "      POLITICS       0.57      0.87      0.69       144\n",
      "\n",
      "      accuracy                           0.65       751\n",
      "     macro avg       0.63      0.64      0.61       751\n",
      "  weighted avg       0.67      0.65      0.64       751\n",
      "\n",
      "\n",
      "Cluster Interpretation:\n",
      "Cluster 1: 65 documents\n",
      "Cluster 2: 229 documents\n",
      "Cluster 3: 238 documents\n",
      "Cluster 4: 219 documents\n"
     ]
    }
   ],
   "source": [
    "result = cluster_kmean(train_text, test_text, test_label, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EXko9ChSGA-g",
    "outputId": "0e7278a7-cfc7-4308-d26d-7c907c64dd26"
   },
   "outputs": [],
   "source": [
    "result = cluster_kmean(train_text, test_text, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "lE-f9ax5TpCt"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the categorical labels to numerical labels\n",
    "numerical_labels = label_encoder.fit_transform(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "yAV_S1g8GA-h"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "def cluster_gmm(train_text, test_text, test_label):\n",
    "\n",
    "    # TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_train = vectorizer.fit_transform(train_text)\n",
    "\n",
    "    # Choosing the number of clusters for GMM\n",
    "    num_clusters_gmm = len(np.unique(test_label))  # Use the number of unique labels in the test set\n",
    "\n",
    "    # Choosing the covariance type for GMM\n",
    "    covariance_type = 'full'  # Experiment with other types if needed\n",
    "\n",
    "    # GMM clustering\n",
    "    gmm = GaussianMixture(n_components=num_clusters_gmm, covariance_type=covariance_type, n_init=10, random_state=42)\n",
    "    gmm.fit(tfidf_train.toarray())  # GMM requires dense input\n",
    "\n",
    "    # Predict cluster labels for test data\n",
    "    tfidf_test = vectorizer.transform(test_text)\n",
    "    predicted_labels_gmm = gmm.predict(tfidf_test.toarray())\n",
    "\n",
    "    # Evaluate the clustering performance for GMM\n",
    "    print(\"GMM Classification Report:\")\n",
    "    print(classification_report(test_label, predicted_labels_gmm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iA2XNwZzGA-h"
   },
   "source": [
    "## Q2: Clustering by Gaussian Mixture Model\n",
    "\n",
    "In this task, you'll re-do the clustering using a Gaussian Mixture Model. Call this function  `cluster_gmm(train_text, test_text, test_label)`.\n",
    "\n",
    "You may take a subset from the data to do GMM because it can take a lot of time.\n",
    "\n",
    "Write your analysis on the following:\n",
    "- How did you pick the parameters such as the number of clusters, variance type etc.?\n",
    "- Compare to Kmeans in Q1, do you achieve better preformance by GMM?\n",
    "\n",
    "- Note, be sure to use different initial means (i.e. `n_init` parameter) when fitting the model to achieve the model stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yIrheatUGA-h"
   },
   "outputs": [],
   "source": [
    "def cluster_gmm(train_text, test_text, test_label):\n",
    "\n",
    "    # Add your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5C32K1KDGA-h"
   },
   "outputs": [],
   "source": [
    "import pandas as pd  # Assuming you have a DataFrame\n",
    "\n",
    "# Assuming your data is in a DataFrame named 'df'\n",
    "# Randomly sample 30% of the data\n",
    "subset_df = train.sample(frac=0.3, random_state=42)\n",
    "subset_df_test = test.sample(frac=0.15, random_state=42)\n",
    "\n",
    "# Extract the relevant columns for clustering (e.g., 'text' and 'label')\n",
    "train_text_subset = subset_df['body'].tolist()\n",
    "test_text_subset = subset_df_test['body'].tolist()\n",
    "test_label_subset = subset_df_test['category']\n",
    "\n",
    "numerical_labels = label_encoder.fit_transform(test_label_subset)\n",
    "\n",
    "# Now you can use the subset for clustering\n",
    "\n",
    "results = cluster_gmm(train_text_subset, test_text_subset, numerical_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zYnrrfOnGA-h",
    "outputId": "99fa026d-dc11-4c9d-e951-3059c0ad2b3f"
   },
   "outputs": [],
   "source": [
    "reuslts = cluster_gmm(train_text, test_text, test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LxDPP62-GA-i"
   },
   "source": [
    "## Q3: Clustering by LDA\n",
    "\n",
    "In this task, you'll re-do the clustering using LDA. Call this function `cluster_lda(train_text, test_text, test_label)`.\n",
    "\n",
    "However, since LDA returns topic mixture for each document, you `assign the topic with highest probability to each test document`, and then measure the performance as in Q1\n",
    "\n",
    "In addition, within the function, please print out the top 30 words for each topic\n",
    "\n",
    "Finally, please analyze the following:\n",
    "- Based on the top words of each topic, could you assign a meaningful name to each topic? In other words, do you think your result can achieve intratopic coherence and intertopic separation?\n",
    "- Although the test subset shows there are 4 clusters, without this information, how do you choose the number of topics?\n",
    "- Among the three models, KMeans, GMM, and LDA, which model performs the best? Can you explain why this model can outperform the others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2PxUeBldGA-i"
   },
   "outputs": [],
   "source": [
    "def cluster_lda(train, test_text, test_label):\n",
    "\n",
    "    # add your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mA_M_yVkGA-i"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the categorical labels to numerical labels\n",
    "numerical_labels = label_encoder.fit_transform(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Sx57UjbGA-i"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def cluster_lda(train_text, test_text, test_label, num_topics=4):\n",
    "\n",
    "\n",
    "    # Count vectorizer (Bag of Words)\n",
    "    vectorizer = CountVectorizer(min_df=4,stop_words='english')\n",
    "    X_train = vectorizer.fit_transform(train_text)\n",
    "\n",
    "    # LDA model\n",
    "    lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "    lda.fit(X_train)\n",
    "\n",
    "    # Transform the test data to topic space\n",
    "    X_test = vectorizer.transform(test_text)\n",
    "    topic_probabilities = lda.transform(X_test)\n",
    "\n",
    "    # Assign the topic with the highest probability to each test document\n",
    "    predicted_topics = topic_probabilities.argmax(axis=1)\n",
    "\n",
    "    # Print top 30 words for each topic\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        top_words_idx = topic.argsort()[:-31:-1]\n",
    "        top_words = [feature_names[i] for i in top_words_idx]\n",
    "        print(f\"Topic {topic_idx + 1}: {', '.join(top_words)}\")\n",
    "\n",
    "    # Evaluate clustering performance\n",
    "    print(\"\\nLDA Classification Report:\")\n",
    "    print(classification_report(test_label, predicted_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GtjWIqxSGA-j",
    "outputId": "26493e96-5b61-49f8-8add-f48f2f424210"
   },
   "outputs": [],
   "source": [
    "cluster_lda(train_text, test_text, numerical_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQ6LmpSrGA-j"
   },
   "source": [
    "## Q5. Bonus:\n",
    "\n",
    "Can you measure the coherence and separation of the clustering results from the three models? Which model performs the best in terms of the coherence and separation?\n",
    "\n",
    "Explain your idea and implment it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v_J0dEz0GA-j",
    "outputId": "e76dda50-1507-4a7d-f3d7-bfef2333a0ce"
   },
   "outputs": [],
   "source": [
    "test_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pMaTjciRGA-k"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
