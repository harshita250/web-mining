{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 6: Sentiment Analysis & Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question, you'll need dataset:\n",
    "- `hw6_train.csv`: dataset fro training\n",
    "- `hw6_test.csv`: dataset for test\n",
    "\n",
    "A snippet of the dataset is given below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 2)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv(\"hw6_train.csv\")\n",
    "test = pd.read_csv(\"hw6_test.csv\")\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: Unsupervised Sentiment Analysis (3 points)\n",
    "\n",
    "- Write a function `analyze_sentiment(docs, labels, th)` as follows: (3 points)\n",
    "    - Takes three inputs:\n",
    "       - `docs` : a list of documents, \n",
    "       - `labels` the ground-truth sentiment labels of `docs`\n",
    "       - `th`: compound threshold\n",
    "    - Use Vader to get a compound score of for each document in `docs`.  \n",
    "    - If `compound score > th`, then the predicted label is 1; otherwise 0\n",
    "    - Print out the classification report\n",
    "    - Return F1 macro score\n",
    "\n",
    "\n",
    "- Tune `th` such that the F1 macro score is maximimized (1 point)\n",
    "- With the `th` tuned, calculate the performance on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, f1_score\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "def analyze_sentiment(docs, labels, th=0):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    # Get compound scores for each document\n",
    "    compound_scores = [sid.polarity_scores(doc)['compound'] for doc in docs]\n",
    "    \n",
    "    # Predict sentiment labels based on the threshold\n",
    "    predicted_labels = [1 if score > th else 0 for score in compound_scores]\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"Classification Report:\\n\", classification_report(labels, predicted_labels))\n",
    "    \n",
    "    # Calculate F1 macro score\n",
    "    f1 = f1_score(labels, predicted_labels, average='macro')\n",
    "    print(\"F1 Macro Score:\", f1)\n",
    "    \n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.72      0.68      9968\n",
      "           1       0.68      0.59      0.63     10032\n",
      "\n",
      "    accuracy                           0.66     20000\n",
      "   macro avg       0.66      0.66      0.66     20000\n",
      "weighted avg       0.66      0.66      0.66     20000\n",
      "\n",
      "F1 Macro Score: 0.6561832243120658\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6561832243120658"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_sentiment(test[\"text\"], test[\"label\"], 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: Supervised Sentiment Analysis Using Word Vectors (7 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.1: Train Word Vectors\n",
    "\n",
    "Write a function `train_wordvec(docs, vector_size)` as follows:\n",
    "- Take two inputs:\n",
    "    - `docs`: a list of documents\n",
    "    - `vector_size`: the dimension of word vectors\n",
    "- First tokenize `docs` into tokens\n",
    "- Use `gensim` package to train word vectors. Set the `vector size` and also carefully set other parameters such as `window`, `min_count` etc.\n",
    "- return the trained word vector model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_wordvec(docs, vector_size = 100):\n",
    "    \n",
    "     # add your code\n",
    "    \n",
    "    return wv_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def train_wordvec(docs, vector_size):\n",
    "    # Tokenize the documents\n",
    "    tokenized_docs = [word_tokenize(doc.lower()) for doc in docs]\n",
    "\n",
    "    # Set parameters for Word2Vec model\n",
    "    window_size = 5  # Maximum distance between the current and predicted word within a sentence\n",
    "    min_word_count = 1  # Ignores all words with a total frequency lower than this\n",
    "    workers = 4  # Number of CPU cores to use while training the model\n",
    "\n",
    "    # Train Word2Vec model\n",
    "    model = Word2Vec(sentences=tokenized_docs, vector_size=vector_size, window=window_size,\n",
    "                     min_count=min_word_count, workers=workers)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec<vocab=71141, vector_size=100, alpha=0.025>\n"
     ]
    }
   ],
   "source": [
    "#wv_model = train_wordvec(train[\"text\"], vector_size = 100)\n",
    "print(wv_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize train and test documents\n",
    "tokenized_train_docs = [word_tokenize(doc.lower()) for doc in train[\"text\"]]\n",
    "tokenized_test_docs = [word_tokenize(doc.lower()) for doc in test[\"text\"]]\n",
    "\n",
    "# Vectorize documents using TFIDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=5)\n",
    "tfidf_train_matrix = tfidf_vectorizer.fit_transform([' '.join(doc) for doc in tokenized_train_docs])\n",
    "tfidf_test_matrix = tfidf_vectorizer.transform([' '.join(doc) for doc in tokenized_test_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 7808)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_test_matrix[:100].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.2: Generate Vector Representation for Documents \n",
    "\n",
    "Write a function `generate_doc_vector(train_docs, test_docs, wv_model, wv_dim= 100, stop_words = None, min_df = 1, topK = None)` as follows:\n",
    "- Take two inputs:\n",
    "    - `train_docs`: a list of train documents, \n",
    "    - `test_docs`: a list of train documents, \n",
    "    - `wv_model`: trained word vector model. \n",
    "    - `wv_dim`: dimensionality of word vector. Set the default value to 100.\n",
    "    - `stop_words`: whether to remove stopwords\n",
    "    - `min_df`: minimum document frequency\n",
    "- First vectorize each document using TFIDF vectorizer by considering stop_words and min_df configurations.\n",
    "- For each token in the vocabulary, look up for its word vector in `wv_model`. \n",
    "- Then calculate the document vector (denoted as `d`) of `doc` by the following methods:\n",
    "    - if `topK` is None, `d` is the `TFIDF-weighted sum of the word vectors of its tokens`, i.e. $d = \\frac{1}{\\sum{tfidf_i}} * \\sum_{i \\in doc}{tfidf_i * v_i}$, where $v_i$ is the word vector of the i-th token, and $tfidf_i$ is the tfidf weigth of this token.\n",
    "    - Otherwise, `d` is the average word vectors of words with topK tfidf weights, i.e.,\n",
    "    $d =   \\frac{1}{K} * \\sum_{i \\in doc, k\\in {topK}}{ v_{i,k}}$, where $topK$ is a parameter.\n",
    "- Return the vector representations of all `train_docs` as a numpy array of shape `(n, vector_size)`, where `n` is the number of documents in `train_docs` and `vector_size` is the dimension of word vectors. Create similar representations for `test_docs`.\n",
    "\n",
    "\n",
    "Note: It may not be a good idea to represent a document as the weighted sum of its word vectors. For example, if one word is positive and another is negative, the sum of the these two words may make the resulting vector is no longer sensitive to sentiment. You'll learn more advanced methods to generate document vector in deep learning courses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def generate_doc_vector(train_docs, test_docs, wv_model, wv_dim=100, stop_words=None, min_df=5, topK=None):\n",
    "    # Tokenize train and test documents\n",
    "    tokenized_train_docs = [word_tokenize(doc.lower()) for doc in train_docs]\n",
    "    tokenized_test_docs = [word_tokenize(doc.lower()) for doc in test_docs]\n",
    "\n",
    "    # Configure stop words\n",
    "    if stop_words is not None:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Vectorize documents using TFIDF vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words, min_df=min_df)\n",
    "    tfidf_train_matrix = tfidf_vectorizer.fit_transform([' '.join(doc) for doc in tokenized_train_docs])\n",
    "    tfidf_test_matrix = tfidf_vectorizer.transform([' '.join(doc) for doc in tokenized_test_docs])\n",
    "\n",
    "    # Get word vectors for each token in the vocabulary\n",
    "    word_vectors = []\n",
    "    for token in tfidf_vectorizer.get_feature_names_out():\n",
    "        if token in wv_model.wv:\n",
    "            word_vectors.append(wv_model.wv[token])\n",
    "        else:\n",
    "            word_vectors.append(np.zeros(wv_dim))  # Use zero vector for unknown words\n",
    "\n",
    "    word_vectors = np.array(word_vectors)\n",
    "\n",
    "    # Generate document vectors\n",
    "    doc_vectors_train = []\n",
    "    doc_vectors_test = []\n",
    "\n",
    "    for i in range(len(tokenized_train_docs)):\n",
    "        tfidf_weights = tfidf_train_matrix[i:100]\n",
    "\n",
    "        if topK is None:\n",
    "            doc_vector = np.sum(tfidf_weights * word_vectors, axis=0) / np.sum(tfidf_weights)\n",
    "        else:\n",
    "            top_indices = np.argsort(tfidf_weights)[-topK:]\n",
    "            doc_vector = np.mean(word_vectors[top_indices], axis=0)\n",
    "\n",
    "        doc_vectors_train.append(doc_vector)\n",
    "\n",
    "    for i in range(len(tokenized_test_docs)):\n",
    "        tfidf_weights = tfidf_test_matrix[i:100]\n",
    "\n",
    "        if topK is None:\n",
    "            doc_vector = np.sum(tfidf_weights * word_vectors, axis=0) / np.sum(tfidf_weights)\n",
    "        else:\n",
    "            top_indices = np.argsort(tfidf_weights)[-topK:]\n",
    "            doc_vector = np.mean(word_vectors[top_indices], axis=0)\n",
    "\n",
    "        doc_vectors_test.append(doc_vector)\n",
    "\n",
    "    return np.array(doc_vectors_train), np.array(doc_vectors_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_doc_vector(train_docs, test_docs, wv_model, wv_dim= 100,\n",
    "                        stop_words = None, min_df = 1, topK=None):\n",
    "    \n",
    "    # add your code\n",
    "    return train_vec, test_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sg/3tj_k0kn1315z20ckp9dznph0000gn/T/ipykernel_4090/618547814.py:38: RuntimeWarning: invalid value encountered in divide\n",
      "  doc_vector = np.sum(tfidf_weights * word_vectors, axis=0) / np.sum(tfidf_weights)\n",
      "/var/folders/sg/3tj_k0kn1315z20ckp9dznph0000gn/T/ipykernel_4090/618547814.py:49: RuntimeWarning: invalid value encountered in divide\n",
      "  doc_vector = np.sum(tfidf_weights * word_vectors, axis=0) / np.sum(tfidf_weights)\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X = generate_doc_vector(train[\"text\"], test[\"text\"], \n",
    "                                      wv_model, wv_dim= 100,\n",
    "                                      stop_words = None, min_df = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.3: Put everything together\n",
    "\n",
    "\n",
    "Define a function `predict_sentiment(train_text, train_label, test_text, test_label, wv_model, wv_dim= 100, stop_words = None, min_df = 1)` as follows:\n",
    "\n",
    "- Take the following inputs:\n",
    "    - `train_text, train_label`: a list of documents and their labels for training\n",
    "    - `test_text, test_label`: a list of documents and their labels for testing,\n",
    "    - `wv_model`: trained word vector model. \n",
    "    - `wv_dim`: dimensionality of word vector. Set the default value to 100.\n",
    "    - `stop_words`: whether to remove stopwords\n",
    "    - `min_df`: minimum document frequency\n",
    "- Call `generate_doc_vector` to generate vector representations (denoted as `train_X` and `test_X`) for documents in `train_text` and `test_text`. \n",
    "- Fit a linear SVM model using `train_X` and `train_label`\n",
    "- Predict the label for `test_X` and print out classification report for the testing subset.\n",
    "- This function has no return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.4: Analysis \n",
    "\n",
    "- Compare the classification reports you obtain from Q1 and Q2.3. Which model performs better?\n",
    "- Why this model can achieve better performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(train_text, train_label, \n",
    "                      test_text, test_label, \n",
    "                      wv_model, wv_dim= 100,\n",
    "                      stop_words = None, min_df = 1, topK = None):\n",
    "    \n",
    "    # Add your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear].........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*...............................................................................................................................................................................................................\n",
      "optimization finished, #iter = 10000\n",
      "\n",
      "WARNING: reaching max number of iterations\n",
      "Using -s 2 may be faster (also see FAQ)\n",
      "\n",
      "Objective value = -44181.784763\n",
      "nSV = 55415\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.70      0.72      9968\n",
      "           1       0.71      0.74      0.73     10032\n",
      "\n",
      "    accuracy                           0.72     20000\n",
      "   macro avg       0.72      0.72      0.72     20000\n",
      "weighted avg       0.72      0.72      0.72     20000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rliu20/miniconda3/envs/pytorch/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "predict_sentiment(train[\"text\"], train[\"label\"],\\\n",
    "                  test[\"text\"], test[\"label\"],\\\n",
    "                  wv_model, wv_dim= 100,\n",
    "                  stop_words = None, min_df = 5, topK = None)\n",
    "\n",
    "\n",
    "predict_sentiment(train[\"text\"], train[\"label\"],\\\n",
    "                  test[\"text\"], test[\"label\"],\\\n",
    "                  wv_model, wv_dim= 100,\n",
    "                  stop_words = None, min_df = 5, topK = 10)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
