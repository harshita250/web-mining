{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>HW #1</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">Each assignment needs to be completed independently. Never ever copy others' work (even with minor modification, e.g. changing variable names). Anti-Plagiarism software will be used to check all submissions. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions**: \n",
    "- Please read the problem description carefully\n",
    "- Make sure to complete all requirements (shown as bullets). In general, it would be much easier if you complete the requirements in the order as shown in the problem description\n",
    "- Follow the Submission Instruction to submit your assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem Description**\n",
    "\n",
    "In this assignment, you'll write functions to analyze an article to find out the word distributions and key concepts. \n",
    "\n",
    "The packages you'll need for this assignment include numpy and pandas. You may need the following functions:\n",
    "- string/list functions: `split`, `replace`, `count`,`index`, `zip`\n",
    "- numpy functions: `sum`, `mean`, `where`, `argsort`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Define a function to analyze word counts in an input sentence\n",
    "\n",
    "\n",
    "Define a function named `tokenize(text)` which does the following: \n",
    "* accepts a sentence (i.e., `text` parameter) as an input\n",
    "* replaces each punctuation by a space\n",
    "    - e.g., `it's a hello world!!!` -> `it s a hello world   `\n",
    "    - hint, you can import module *string*, use `string.punctuation` to get a list of punctuations\n",
    "* splits the sentence into a list of tokens by **space** (including tab, and new line). \n",
    "    - e.g., `it s a hello world   ` will be split into tokens `[\"it\" \"s\", \"a\",\"hello\",\"world\"]`  \n",
    "* strips the **leading/trailing spaces** of each token, if any \n",
    "* removes any empty tokens\n",
    "* only keeps tokens with 2 or more characters, i.e. `len(token)>1`  \n",
    "* converts all tokens into lower case \n",
    "* find the count of each unique token and save the counts as dictionary, i.e., `{world: 1, it: 1, ...}` \n",
    "* returns the dictionary \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \n",
    "    vocab = {}\n",
    "    \n",
    "    # add your code here\n",
    "    \n",
    "    \n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'again': 1, 'is': 1, 'world': 2, 'it': 2, 'hello': 2}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test your code\n",
    "text = \"\"\"it's a hello world!!!\n",
    "           it is hello world again.\"\"\"\n",
    "tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Generate a document term matrix (DTM) as a numpy array \n",
    "\n",
    "\n",
    "Define a function `get_dtm(sents, min_count = 1)` as follows:\n",
    "- accepts a list of sentences, i.e., `sents`, as an input\n",
    "- uses `tokenize` function you defined in Q1 to get the count dictionary for each sentence (0.5 point)\n",
    "- pools the words from all the strings togehter to get a list of  unique words, denoted as `unique_words` (1 point)\n",
    "- creates a numpy array, say `dtm` with a shape (# of docs x # of unique words), and set the initial values to 0. (0.5 point)\n",
    "- fills cell `dtm[i,j]` with the count of the `j`th word in the `i`th sentence (1 point)\n",
    "- if `min_count > 1` :\n",
    "    - find the words which appear at least `min_count` times in all the sentences (hint: think about array `sum` and `where`)\n",
    "    - only keep the columns correponding to these words and remove the other columns in `dtm`\n",
    "    - update the `unique_words` list to contain only these words.\n",
    "- returns `dtm` and `unique_words` as numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dtm(sents, min_count = 1):\n",
    "    \n",
    "    dtm, all_words = None, None\n",
    "    \n",
    "    # add your code here\n",
    "    \n",
    "    return dtm, all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = [\"it's a hello world!!!\",\n",
    "         \"it is hello world again.\",\n",
    "         \"world-wide problem\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 1., 0., 1., 0., 1.],\n",
       "        [1., 1., 1., 0., 1., 0., 1.],\n",
       "        [0., 0., 1., 1., 0., 1., 0.]]),\n",
       " array(['again', 'is', 'world', 'problem', 'it', 'wide', 'hello'],\n",
       "       dtype='<U7'))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_dtm(sents, min_count = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 0., 0.]]),\n",
       " array(['world', 'it', 'hello'], dtype='<U7'))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_dtm(sents, min_count = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3 Analyze DTM Array \n",
    "\n",
    "\n",
    "**Don't use any loop in this task**. You should use array operations to take the advantage of high performance computing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function named `analyze_dtm(dtm, words)` which:\n",
    "* takes an array $dtm$ and $words$ as an input, where $dtm$ is the array you get in Q2 with a shape $(m \\times n)$, and $words$ contains an array of words corresponding to the columns of $dtm$.\n",
    "* calculates the sentence frequency for each word, say $j$, e.g. how many sentences contain word $j$. Save the result to array $df$ ($df$ has shape of $(n,)$ or $(1, n)$). \n",
    "* normalizes the word count per sentence: divides word count, i.e., $dtm_{i,j}$, by the total number of words in sentence $i$. Save the result as an array named $tf$ ($tf$ has shape of $(m,n)$). \n",
    "* for each $dtm_{i,j}$, calculates $tf\\_idf_{i,j} = \\frac{tf_{i, j}}{1+log(df_j)}$, i.e., divide each normalized word count by the log of the sentence frequency of the word (we'll explain this formula later). $tf\\_idf$ has shape of $(m,n)$ \n",
    "* prints out the following:\n",
    "    \n",
    "    - the total number of words in the document represented by $dtm$ \n",
    "    - the average number of words per sentence\n",
    "    - the most frequent top 10 words in this document    \n",
    "    - words with the top 10 largest $df$ values (show words and their $df$ values) \n",
    "    - the longest sentence (i.e., the one with the most words) \n",
    "    - top-10 words with the largest $tf\\_idf$ values in the longest sentence (show words and values) \n",
    "* returns the $tf\\_idf$ array.\n",
    "\n",
    "\n",
    "Test\n",
    "- Test it using the `dtm` generated with `min_count = 1` and `min_count = 2`, respectively\n",
    "- Note that `min_count` can change the top 10 words with highest tf-idf values in the longest sentece. Can you explain why? Type your answer as a markdown.\n",
    "\n",
    "\n",
    "Note, \n",
    "- for all the steps, **do not use any loop**. Just use array functions and broadcasting for high performance computation.\n",
    "- A test document has been provided to you. This article can be found at https://hbr.org/2022/04/the-power-of-natural-language-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dtm(dtm, words, sents):\n",
    "    \n",
    "    # add your code here \n",
    "    #Use zip to print two numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Power of Natural Language Processing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Until recently, the conventional wisdom was th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>But in the past two years language-based AI ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The most visible advances have been in what’s ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It has been used to write an article for The G...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0           The Power of Natural Language Processing\n",
       "1  Until recently, the conventional wisdom was th...\n",
       "2  But in the past two years language-based AI ha...\n",
       "3  The most visible advances have been in what’s ...\n",
       "4  It has been used to write an article for The G..."
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load test article\n",
    "\n",
    "sents = pd.read_csv(\"sents.csv\")\n",
    "sents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of words:\n",
      "1884.0\n",
      "\n",
      "The top 10 frequent words:\n",
      "[('the', 69.0), ('to', 66.0), ('and', 53.0), ('of', 50.0), ('for', 37.0), ('ai', 29.0), ('in', 24.0), ('is', 23.0), ('are', 22.0), ('data', 21.0)]\n",
      "\n",
      "Average words per sentence is 23.26 \n",
      "\n",
      "The top 10 words with highest df values:\n",
      "[('the', 47), ('to', 42), ('and', 41), ('of', 38), ('for', 32), ('ai', 26), ('in', 22), ('is', 20), ('like', 20), ('tasks', 19)]\n",
      "\n",
      "The longest sentence :\n",
      "Language models are already reshaping traditional text analytics, but GPT-3 was an especially pivotal language model because, at 10x larger than any previous model upon release, it was the first large language model, which enabled it to perform even more advanced tasks like programming and solving high school–level math problems.\n",
      "\n",
      "The top 10 words with highest tf-idf values in the longest sentece:\n",
      "[('model', 0.020367219964003532), ('problems', 0.02), ('pivotal', 0.02), ('release', 0.02), ('10x', 0.02), ('school–level', 0.02), ('upon', 0.02), ('enabled', 0.02), ('larger', 0.02), ('math', 0.02)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test 1: min_count = 1\n",
    "\n",
    "dtm, words = get_dtm(sents.text, min_count = 1)\n",
    "\n",
    "tfidf = analyze_dtm(dtm, words, sents.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of words:\n",
      "1474.0\n",
      "\n",
      "The top 10 frequent words:\n",
      "[('the', 69.0), ('to', 66.0), ('and', 53.0), ('of', 50.0), ('for', 37.0), ('ai', 29.0), ('in', 24.0), ('is', 23.0), ('are', 22.0), ('like', 21.0)]\n",
      "\n",
      "Average words per sentence is 18.20 \n",
      "\n",
      "The top 10 words with highest df values:\n",
      "[('the', 47), ('to', 42), ('and', 41), ('of', 38), ('for', 32), ('ai', 26), ('in', 22), ('is', 20), ('like', 20), ('tasks', 19)]\n",
      "\n",
      "The longest sentence :\n",
      "It is difficult to anticipate just how these tools might be used at different levels of your organization, but the best way to get an understanding of this tech may be for you and other leaders in your firm to adopt it yourselves.\n",
      "\n",
      "The top 10 words with highest tf-idf values in the longest sentece:\n",
      "[('to', 0.01623647977187337), ('different', 0.015144002798708749), ('leaders', 0.015144002798708749), ('way', 0.015144002798708749), ('difficult', 0.015144002798708749), ('it', 0.01438507146664413), ('your', 0.013378345184352776), ('be', 0.013181786850566587), ('best', 0.012218086103602675), ('might', 0.012218086103602675)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test 2: min_count = 2\n",
    "\n",
    "dtm, words = get_dtm(sents.text, min_count = 2)\n",
    "\n",
    "tfidf = analyze_dtm(dtm, words, sents.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. Subword tokenization (Bonus) \n",
    "\n",
    "You may find your tokens contain inflections (e.g., 'generate', 'generating'). Can you design an algorithm which:\n",
    "- start building a vocabulary with single unique characters as tokens, \n",
    "- then iteratively merge the most frequently cooccuring token pair as a new token and add it to your vocabulary\n",
    "- keep merging until you reach the maximum number of tokens allowed for your vocabulary.\n",
    "\n",
    "For example, suppose you have the list of tokens below \n",
    "\n",
    " ['game', 'games','general','generalizable','generation']\n",
    " \n",
    "You first treat each letter as a token in your vocabulary. Then you find the most freqent token pair in the words, which can be \"g\" and \"e\". Then you add \"ge\" as a new token to your vocabulary. The next merge can be \"ge\" and \"n\", and so on. \n",
    " \n",
    "At the end, you split each word into one more tokens in your vocabulary. Please minimize the number of tokens used for each word. For exmaple, \"general\" may be split into \"genera\" and \"l\" in this example.\n",
    "  \n",
    "Describe your ideas as markdowns and also implement your ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put everything together and test using main block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question 1\n",
      "{'again': 1, 'is': 1, 'world': 2, 'it': 2, 'hello': 2}\n",
      "\n",
      "Test Question 2\n",
      "(3, 7)\n",
      "\n",
      "Test Question 3\n",
      "The total number of words:\n",
      "1884.0\n",
      "\n",
      "The top 10 frequent words:\n",
      "[('the', 69.0), ('to', 66.0), ('and', 53.0), ('of', 50.0), ('for', 37.0), ('ai', 29.0), ('in', 24.0), ('is', 23.0), ('are', 22.0), ('data', 21.0)]\n",
      "\n",
      "Average words per sentence is 23.26 \n",
      "\n",
      "The top 10 words with highest df values:\n",
      "[('the', 47), ('to', 42), ('and', 41), ('of', 38), ('for', 32), ('ai', 26), ('in', 22), ('is', 20), ('like', 20), ('tasks', 19)]\n",
      "\n",
      "The longest sentence :\n",
      "Language models are already reshaping traditional text analytics, but GPT-3 was an especially pivotal language model because, at 10x larger than any previous model upon release, it was the first large language model, which enabled it to perform even more advanced tasks like programming and solving high school–level math problems.\n",
      "\n",
      "The top 10 words with highest tf-idf values in the longest sentece:\n",
      "[('model', 0.020367219964003532), ('problems', 0.02), ('pivotal', 0.02), ('release', 0.02), ('10x', 0.02), ('school–level', 0.02), ('upon', 0.02), ('enabled', 0.02), ('larger', 0.02), ('math', 0.02)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# best practice to test your class\n",
    "# if your script is exported as a module,\n",
    "# the following part is ignored\n",
    "# this is equivalent to main() in Java\n",
    "\n",
    "if __name__ == \"__main__\":  \n",
    "    \n",
    "    # Test Question 1\n",
    "    text = \"\"\"it's a hello world!!!\n",
    "           it is hello world again.\"\"\"\n",
    "    print(\"Test Question 1\")\n",
    "    print(tokenize(text))\n",
    "    \n",
    "    \n",
    "    # Test Question 2\n",
    "    print(\"\\nTest Question 2\")\n",
    "    sents = [\"it's a hello world!!!\",\n",
    "         \"it is hello world again.\",\n",
    "         \"world-wide problem\"]\n",
    "    \n",
    "    dtm, all_words = get_dtm(sents, min_count = 1)\n",
    "    print(dtm.shape)\n",
    "    \n",
    "    \n",
    "    #3 Test Question 3\n",
    "    \n",
    "    print(\"\\nTest Question 3\")\n",
    "    sents = pd.read_csv(\"sents.csv\")\n",
    "    dtm, all_words = get_dtm(sents.text, min_count = 1)\n",
    "    tfidf= analyze_dtm(dtm, words, sents.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission guideline\n",
    "\n",
    "1. When you complete your code, please answer all analysis questions as markdowns\n",
    "2. Please generate a pdf file out of the notebook using `File -> Print` and print as pdf.\n",
    "3. Then you convert your notedbook into executable .py file using `File -> Save and Export Notebook As`, and then choose `Executable Scripts`. A \".py\" file should be generated. \n",
    "4. Open a command or terminal window to run .py file by issuing `python xx.py`. You should see all printout.\n",
    "5. Upload both the pdf and .py files to Canvas.\n",
    "\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
