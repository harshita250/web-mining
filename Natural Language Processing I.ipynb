{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Natural Language Processing Using NLTK (I)</center>\n",
    "\n",
    "References:\n",
    " - http://www.nltk.org/book_1ed/\n",
    " - https://web.stanford.edu/class/cs124/lec/Information_Extraction_and_Named_Entity_Recognition.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. NLTK installation\n",
    " 1. Install NLTK package using: pip install nltk \n",
    " 2. Open your python editor (Jupyter Notebook, Spyder etc.) and type the following comands below. Select \"all packages\" to install data included in NLTK, including corpora and books. It may take a few minutes to download all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. NLP Objectives and Basic Steps\n",
    "\n",
    " - Objectives:\n",
    "   * Split documents into tokens, phrases, or segments\n",
    "   * Clean up tokens and annotate tokens\n",
    "   * Extract features from tokens for further text mining tasks\n",
    " - Basic processing steps:\n",
    "   * Tokenization: split documents into individual words, phrases, or segments\n",
    "   * Remove stop words and filter tokens\n",
    "   * POS (part of speech) Tagging\n",
    "   * Normalization: Stemming, Lemmatization\n",
    "   * Named Entity Recognition (NER)\n",
    "   * Term Frequency and Inverse Dcoument Frequency (TF-IDF)\n",
    "   * Document-to-term matrix (bag of words)\n",
    " - NLP packages: NLTK, Gensim, spaCy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import re    # import re module\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this extract is from https://www.sciencenews.org/article/coronavirus-what-does-covid-19-vaccine-efficacy-mean\n",
    "\n",
    "text = \"The FDA setting a minimum recommendation for efficacy doesn't mean vaccines \\\n",
    "couldn't perform better. The benchmark is also a reminder that COVID-19 vaccine \\\n",
    "development is in its early days. If the first vaccines made available only meet \\\n",
    "the minimum, they may be replaced by others that prove to protect more people. \\\n",
    "But with more than 1 million deaths from COVID-19 worldwide — \\\n",
    "and U.S. deaths surpassing 200,000 — the urgency in finding a \\\n",
    "vaccine that safely helps at least some people is at the forefront.\"\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Tokenization\n",
    " - **Definition**: the process of breaking a stream of textual content up into words, terms, symbols, or some other meaningful elements called tokens.\n",
    "    * Word (Unigram)\n",
    "    * Bigram (Two consecutive words)\n",
    "    * Trigram (Three consecutive words)\n",
    "    * Sentence\n",
    " - Different methods exist:\n",
    "    * Split by regular expression patterns\n",
    "    * NLTK's word tokenizer\n",
    "    * NLTK's regular expression tokenizer (customizable)\n",
    " - None of them can be perfect for any tokenization task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Unigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.1.1. Simply split the text by one or more non-word characters\n",
    "\n",
    "# \\W+: one or more non-words\n",
    "tokens = re.split(r\"\\W+\", text)   \n",
    "\n",
    "# get the number of tokens\n",
    "\n",
    "print(len(tokens))                   \n",
    "print(tokens)                     \n",
    "\n",
    "# Pros: no punctuation, just words\n",
    "# Cons: COVID-19, doesn't, couldn't, 200,000\n",
    "# are split into two words\n",
    "\n",
    "re.findall(r\"\\w+\", text) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK's word tokenizer does the following steps:\n",
    "* split standard contractions, e.g. don't -> do n't and they'll -> they 'll\n",
    "* treat most punctuation characters as separate tokens\n",
    "* split off commas and single quotes, when followed by whitespace\n",
    "* separate periods that appear at the end of line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.1.2 NLTK's word tokenizer: \n",
    "\n",
    "# break down text into words and punctuations\n",
    "\n",
    "# invoke NLTK's word tokenizer\n",
    "tokens = nltk.word_tokenize(text)    \n",
    "print(len(tokens) )                   \n",
    "print (tokens)       \n",
    "\n",
    "# Pros: words are well tokenized, \n",
    "# e.g. COVID-19, 200,000 are not split by punctuations\n",
    "# doesn't becomes does n't\n",
    "# cons: need to remove punctuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.1.3 remove leading or trailing punctuations\n",
    "\n",
    "import string\n",
    "\n",
    "string.punctuation\n",
    "\n",
    "tokens=[token.strip(string.punctuation+'—') for token in tokens]\n",
    "tokens\n",
    "# remove empty tokens\n",
    "tokens=[token.strip() for token in tokens \\\n",
    "        if token.strip()!='']\n",
    "print(len(tokens) )\n",
    "print(tokens)  \n",
    "\n",
    "# Note '—' is still kept since it's not in the punctuation list. How to remove it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK's regular expression tokinizer (customizable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.1.4 NLTK's regular expression tokenizer \n",
    "\n",
    "# Pattern can be customized to your need\n",
    "\n",
    "# a word is defined as:\n",
    "# (1) must start with a word character  \\w\n",
    "# (2) then contain zero or more word characters,\"-\",\",\", \n",
    "#     or \"'\" in the middle [\\w\\,'-]*\n",
    "#     e.g.: couldn't, 600,000, COVID-19\n",
    "# (3) must end with a word character \\w\n",
    "\n",
    "pattern=r'\\w[\\w\\',-]*\\w'                        \n",
    "\n",
    "# call NLTK's regular expression tokenization\n",
    "tokens=nltk.regexp_tokenize(text, pattern)\n",
    "\n",
    "print(len(tokens))\n",
    "print (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise use regular expression tokenizer to extract\n",
    "# course and title pharse, i.e \n",
    "# 'COM-101 COMPUTERS'\n",
    "\n",
    "text = '''COM-101   COMPUTERS\n",
    "COM-111   DATABASE\n",
    "COM-211   ALGORITHM\n",
    "MAT-103   STATISTICS learning\n",
    "MAT-102   STATISTICS'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.2.1. Segmentation by Sentences\n",
    "\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "len(sentences)\n",
    "sentences\n",
    "\n",
    "# what patterns can be used to segment \n",
    "# text into sentences?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Phrases: Bigrams (2 consecutive words),  Trigrams (3 consecutive words), or in general n-grams\n",
    " - Why bigrams and trigrams?\n",
    " - How to get bigrams or trigrams:\n",
    "    1. First tokenize text into unigrams\n",
    "    2. Slice through the list of unigrams to get bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.3.1. Get bigrams from the text                       \n",
    "\n",
    "# bigrams are formed from unigrams\n",
    "# nltk.bigram returns an iterator\n",
    "\n",
    "bigrams=list(nltk.bigrams(tokens))  # tokens are created in Exercise 3.1.4\n",
    "print(bigrams)\n",
    "\n",
    "# trigrams\n",
    "list(nltk.trigrams(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Collocation\n",
    " - Most bigrams or trigrams may sound odd. However, we need to pay attention to frequent bigrams or trigrams\n",
    " - **Collocation**: an expression consisting of two or more words that correspond to some conventional way of saying things, e.g. red wine, United States, balance sheet etc.\n",
    "    - Collocations are not fully compositional in that there is usually an element of meaning added to the combination.\n",
    " - Question: how to find collocations?\n",
    "    - Suppose you have a rich collection of text, e.g. english-web.txt\n",
    "    - How to find good collocations from this file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text: inaugural address\n",
    "\n",
    "# To check the text, use\n",
    "\n",
    "print(nltk.corpus.inaugural.raw('1789-Washington.txt')[0:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.4.1.\n",
    "# construct bigrams using words from a large bulit-in NLTK corpus\n",
    "\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "\n",
    "# bigram association measures\n",
    "# different measures, e.g. frequency, are implemented\n",
    "\n",
    "bigram_measures = BigramAssocMeasures()\n",
    "\n",
    "# First load text from a NLTK corpus (inagural) \n",
    "# and create unigram tokens\n",
    "# Then create bigrams from the tokens\n",
    "words=nltk.corpus.inaugural.words()\n",
    "\n",
    "finder = BigramCollocationFinder.from_words(words)\n",
    "\n",
    "# find the top 10 bigrams by frequency\n",
    "finder.nbest(bigram_measures.raw_freq, 10) \n",
    "\n",
    "# Note that the most frequent bigrams are very odd\n",
    "# how to fix it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.4.2. Find collocation by filter\n",
    "\n",
    "import string\n",
    "# construct bigrams using words from a NLTK corpus\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "#print(stop_words)\n",
    "\n",
    "finder.apply_word_filter(lambda w: w.lower() in stop_words\\\n",
    "                         or w.strip(string.punctuation)=='')\n",
    "\n",
    "finder.nbest(bigram_measures.raw_freq, 20) \n",
    "\n",
    "# better?\n",
    "# notice \"let us\", \"upon us\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 How to find collocations - PMI\n",
    "- By **frequency** (perhaps with filter)\n",
    "- **Pointwise Mutual Information (PMI)**\n",
    "  - giving two words $w_1, w_2$, $$PMI(w_1,w_2)=\\log{\\frac{p(w_1,w_2)}{p(w_1)*p(w_2)}}$$\n",
    "  - Some observations:\n",
    "    - if $w_1$ and $w_2$ are independent, $PMI(w_1,w_2)=0$\n",
    "    - if $w_1$ is completely dependent on $w_2$, i.e. $p(w_1,w_2)=p(w_2)$, $PMI(w_1,w_2)=\\log\\frac{1}{p(w_1)}$. In this case, what if $w_1$ just appears once in the corpus? \n",
    "    - PMI favors less frequent collocations \n",
    "    - how to fix it?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.4.1.1 Metrics for Collocations\n",
    "\n",
    "from nltk.collocations import *\n",
    "\n",
    "# construct bigrams using words from a NLTK corpus\n",
    "finder = BigramCollocationFinder.from_words(words)\n",
    "\n",
    "# find top-n bigrams by pmi\n",
    "finder.nbest(bigram_measures.pmi, 20) \n",
    "\n",
    "# Notice most of them are names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4.1.2 filter bigrams by frequency\n",
    "\n",
    "finder.apply_freq_filter(5)  #5\n",
    "finder.nbest(bigram_measures.pmi, 20) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 How to find collocations - NPMI and others\n",
    "- **Normalized Pointwise Mutual Information (`NPMI`)**\n",
    "   - If $w_1$ and $w_2$ always occur together, i.e., $p(w_1)=p(w_2)=p(w_1,w_2)$, PMI reaches the maximum: $$PMI(w_1,w_2)=-\\log{p(w_1)}=-\\log{p(w_2)}=-\\log{p(w_1,w_2)}$$\n",
    "   - Normalized PMI is the PMI divided by the upper bound:\n",
    "   $$NPMI(w_1,w_2)=\\frac{\\log{\\frac{p(w_1,w_2)}{p(w_1)*p(w_2)}}}{-\\log{p(w_1,w_2)}}$$\n",
    "   \n",
    "- Another simple method by Mikolov et al. (2013) (https://arxiv.org/pdf/1310.4546.pdf):\n",
    "\n",
    "    - $Score(w_1, w_2)=\\frac{count(w_1,w_2)-\\delta}{count(w_1)*count(w_2)}, \\text{where}~\\delta~\\text{is the minimum collocation frequency} $ \n",
    "\n",
    "    - This is equivalent to PMI with a minimum collocation threshold\n",
    "- Both methods are implemented in `gensim` package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3.5. Vocabulary \n",
    " - Vocabulary: the set of unique tokens (unigrams/phrases)  \n",
    " - Dictionary: typicallly, the vocabulary of a text can be represented as a dictionary \n",
    "    * Key: word, Value: count of the word\n",
    "    * **nltk.FreqDist()**: a nice function for calculating frequncy of words/phrases\n",
    "        - Get the frequency of items in the parameter list \n",
    "        - Retruns an object similar to a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.5.1 Get token frequency\n",
    "\n",
    "# first tokenize the text\n",
    "pattern=r'\\w[\\w\\',-]*\\w'                        \n",
    "tokens=nltk.regexp_tokenize(text.lower(), pattern)\n",
    "\n",
    "#tokens\n",
    "# get unigram frequency \n",
    "# recall, you can also get the dictionary by \n",
    "# {token:count(token) for token in set(tokens)}\n",
    "\n",
    "word_dist=nltk.FreqDist(tokens)\n",
    "word_dist\n",
    "\n",
    "# get the most frequent items\n",
    "print(\"top 5 words:\", word_dist.most_common(5))\n",
    "\n",
    "# what kind of words usually have high frequency?\n",
    "\n",
    "# it behaves as a dictionary\n",
    "for word in word_dist:\n",
    "    print(word,\":\", word_dist[word])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3.5.1 Stop words and word filtering\n",
    "\n",
    " - Stop words: a set of commonly used words, have very little meaning, and cannot differentiate a text from others, such as \"and\", \"the\" etc. \n",
    " - Stop words are typically ignored in NLP processing or by search engine\n",
    " - Stop words usually are application specific. You can define your own stop words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Exercise 3.5.1.1\n",
    "# get NLTK English stop words\n",
    "# You can modify this list by adding more stop words or remove stop words\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words+=[\"covid-19\", \"virus\"]\n",
    "#print (stop_words)\n",
    "\n",
    "# filter stop words out of the dictionary\n",
    "# by creating a new dictionary\n",
    "\n",
    "filtered_dict={word: word_dist[word] \\\n",
    "                     for word in word_dist \\\n",
    "                     if word not in stop_words}\n",
    "\n",
    "\n",
    "filtered_dict\n",
    "\n",
    "# how to sort the dictionary by value?\n",
    "sorted(filtered_dict.items(), lambda item: -item[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5.2 positive/negative words: sentiment analysis\n",
    "- Sentiment analysis often relies on **lists of words and phrases with positive and negative connotations**. \n",
    "- Many dictionaries of positive and negative opinion words were already developed:\n",
    "\n",
    "  - **Hu and Liu's lexicon**: http://www.cs.uic.edu/~liub/FBS/\n",
    "  - **SentiWordNet**: an excellent publicly available lexicon (http://sentiwordnet.isti.cnr.it/) \n",
    "  - **SentiWords**: contains 155,000 English words (https://hlt-nlp.fbk.eu/technologies/sentiwords)\n",
    "  - **WordStat**: contains more than 9164 negative and 4847 positive word patterns (https://provalisresearch.com/products/content-analysis-software/wordstat-dictionary/sentiment-dictionaries/)\n",
    "  - **SenticNet**: provides polarity associated with 50,000 natural language concepts https://sentic.net\n",
    "  - **Sentiment140**:  created from 1.6 million tweets and contains a list of words and their associations with positive and negative sentiment (https://github.com/felipebravom/StaticTwitterSent/tree/master/extra/Sentiment140-Lexicon-v0.1)\n",
    "- Opinion words are <b>domain-specific</b>. (e.g. \"power\" in political domain vs. in engergy sector)\n",
    "  - For example, for financial industry, there are a number of dictionaries for opinion words:\n",
    "     * Harvard's General Inquirer (GI): http://www.wjh.harvard.edu/~inquirer/\n",
    "     * Loughran and McDonald (2015):  https://sraf.nd.edu/textual-analysis/resources/\n",
    "- For description of these lexicons, check https://medium.com/@datamonsters/sentiment-analysis-tools-overview-part-1-positive-and-negative-words-databases-ae35431a470c\n",
    "- Question: **How to select the right lexicon**?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.5.2.1\n",
    "# Find positive words \n",
    "text = '''the problem is that the writers, james cameron and jay cocks , were too ambitious, aiming for a film with social relevance, thrills, and drama. \n",
    " not that ambitious film-making should be discouraged; just that when it fails to achieve its goals, it fails badly and obviously. \n",
    " the film just ends up preachy, unexciting and uninvolving.'''\n",
    "\n",
    "pattern=r'\\w[\\w\\',-]*\\w'                        \n",
    "tokens=nltk.regexp_tokenize(text.lower(), pattern)\n",
    "\n",
    "\n",
    "with open(\"positive-words.txt\",'r') as f:\n",
    "    positive_words=[line.strip() for line in f]\n",
    "\n",
    "#positive_words\n",
    "#print(positive_words)\n",
    "\n",
    "positive_tokens=[token for token in tokens \\\n",
    "                 if token in positive_words]\n",
    "\n",
    "print(positive_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- **Naive sentiment analysis**:\n",
    "  - Find positive/negative words\n",
    "  - If more positive words than negative, then positive\n",
    "  - Otherwise, negative\n",
    "- Note the sentence: \n",
    "  -  \"the problem is that the writers, james cameron and jay cocks , were **<font color=\"red\">too ambitious</font>**, aiming for a film with social relevance, thrills, and drama. **<font color=\"red\">not that ambitious</font>** film-making should be discouraged; just that when it fails to achieve its goals ...\"\n",
    "- How to deal with **negation**?\n",
    "- Some useful rules:\n",
    "    - Negative sentiment: \n",
    "      - negative words not preceded by a negation within $n$ (e.g. three) words in the same sentence.\n",
    "      - positive words preceded by a negation within $n$ (e.g. three) words in the same sentence.\n",
    "    - Positive sentiment (in the similar fashion):\n",
    "      - positive words not preceded by a negation within $n$ (e.g. three) words in the same sentence.\n",
    "      - negative terms following a negation within  $n$ (e.g. three) words in the same sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.5.2.2 # check if a positive word is preceded by negation words\n",
    "# e.g. not, too, n't, no, cannot\n",
    "\n",
    "# this is not an exhaustive list of negation words!\n",
    "negations=['not', 'too', 'n\\'t', 'no', 'cannot', 'neither','nor', 'little','few']\n",
    "tokens = nltk.word_tokenize(text)  \n",
    "\n",
    "#print(tokens)\n",
    "\n",
    "positive_tokens=[]\n",
    "for idx, token in enumerate(tokens):\n",
    "    if token in positive_words:\n",
    "        if idx>0:\n",
    "            if tokens[idx-1] not in negations:\n",
    "                positive_tokens.append(token)\n",
    "        else:\n",
    "            positive_tokens.append(token)\n",
    "\n",
    "\n",
    "print(positive_tokens)\n",
    "\n",
    "# what if a positive word is preceded \n",
    "# by a negation within N words? \n",
    "# e.g. 'does not make any customer happy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
